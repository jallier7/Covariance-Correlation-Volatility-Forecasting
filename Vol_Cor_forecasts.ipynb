{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "import math\n",
    "import seaborn as sns\n",
    "from cvx.covariance.combination import from_ewmas\n",
    "from arch import arch_model\n",
    "from statsmodels.tsa.vector_ar.vecm import coint_johansen\n",
    "from scipy.optimize import minimize\n",
    "from statsmodels.tools.tools import add_constant\n",
    "from statsmodels.regression.linear_model import OLS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ensemble des librairies utilisées\n",
    "from xbbg import blp\n",
    "import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import os\n",
    "import tensorflow as tf\n",
    "from datetime import datetime\n",
    "from dateutil.relativedelta import relativedelta\n",
    "from scipy import stats\n",
    "\n",
    "\n",
    "from pylatex import Document, Section, Subsection, Math, Table, Tabular, Figure, Command, MiniPage\n",
    "from pylatex.utils import NoEscape\n",
    "import subprocess\n",
    "from pylatex.utils import bold\n",
    "from pylatex.package import Package\n",
    "from PIL import Image\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "from plotly.subplots import make_subplots\n",
    "from tensorflow.keras.initializers import GlorotUniform\n",
    "from keras.layers import Dropout\n",
    "from keras.regularizers import l2\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dropout, Dense, Input\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "from statsmodels.regression.linear_model import OLS\n",
    "from statsmodels.tools.tools import add_constant\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv(r'C:\\Users\\33640\\OneDrive\\Documents\\GitHub\\Covariance-Correlation-Volatility-Forecasting\\SP500_daily_returns.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6224, 355)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nan_counts=df.isna().sum()\n",
    "columns_to_keep = nan_counts[nan_counts <= 3].index\n",
    "returns = df[columns_to_keep]\n",
    "returns=returns.dropna()\n",
    "returns.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dates=returns['Date']\n",
    "returns=returns.drop('Date', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def histo_cov(df, date, window):\n",
    "    cov=np.zeros((len(df.columns),len(df.columns)))\n",
    "    for i in range(date-window+1, date+1):\n",
    "        cov+=np.outer(np.array(df.iloc[i,:]),np.array(df.iloc[i,:]))/(window)\n",
    "    covs=pd.DataFrame(cov*250, columns=df.columns, index=df.columns)\n",
    "    return(covs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ewma_cov(df, date, window, beta):\n",
    "    cov=np.zeros((len(df.columns),len(df.columns)))\n",
    "    for i in range(date-window+1, date+1):\n",
    "        cov=(beta)*cov+(1-beta)*np.outer(np.array(df.iloc[i,:]),np.array(df.iloc[i,:]))\n",
    "    covs=pd.DataFrame(cov*250, columns=df.columns, index=df.columns)\n",
    "    return(covs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "halflife_pairs=[(10, 21), (21, 63), (63, 125)]\n",
    "def ewma_cov(df, date, halflife_pairs, window):    \n",
    "    combinator = from_ewmas(df.iloc[max(0,date-1000):date,:],\n",
    "                                halflife_pairs,\n",
    "                                min_periods_vola=window,  # min periods for volatility estimation\n",
    "                                min_periods_cov=window)  # min periods for correlation estimation\n",
    "\n",
    "    # Solve combination problem and loop through combination results to get predictors\n",
    "    covariance_predictors = {}\n",
    "    for predictor in combinator.solve(window=10):  # lookback window for optimization\n",
    "        # From predictor we can access predictor.time, predictor.mean (=0 here),\n",
    "        # predictor.covariance, and predictor.weights\n",
    "        covariance_predictors[predictor.time] = predictor.covariance\n",
    "    return(covariance_predictors[date]*250)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Fit GARCH(1,1) to each asset's returns\n",
    "def fit_garch(returns):\n",
    "    # Rescaler les rendements\n",
    "    returns_rescaled = returns * 100\n",
    "    \n",
    "    model = arch_model(returns_rescaled, vol='Garch', p=1, q=1)\n",
    "    garch_fit = model.fit(disp=\"off\")\n",
    "    \n",
    "    # Revenir à l'échelle d'origine pour les résidus\n",
    "    residuals = garch_fit.resid / 100\n",
    "    volatilities = garch_fit.conditional_volatility / 100\n",
    "    return volatilities, residuals\n",
    "# Step 2: DCC-GARCH log-likelihood function\n",
    "def dcc_garch_log_likelihood(params, residuals):\n",
    "    \"\"\"\n",
    "    Calcule la log-vraisemblance du modèle DCC-GARCH.\n",
    "    \n",
    "    Paramètres:\n",
    "    params (tuple): Paramètres alpha_2 et beta_2.\n",
    "    residuals (np.array): Matrice des résidus standardisés (T x N).\n",
    "    \n",
    "    Retourne:\n",
    "    float: La valeur de la log-vraisemblance négative.\n",
    "    \"\"\"\n",
    "    alpha_2, beta_2 = params\n",
    "    n_assets = residuals.shape[1]\n",
    "    \n",
    "    # Initialisation de Q_bar et Q_t\n",
    "    T = residuals.shape[0]\n",
    "    Q_bar = np.cov(residuals.T)  # Matrice de corrélation inconditionnelle\n",
    "    Q_t = Q_bar.copy()\n",
    "    \n",
    "    log_likelihood = 0\n",
    "    epsilon = 1e-5  # Petit terme de régularisation pour éviter les matrices singulières\n",
    "    \n",
    "    for t in range(T):\n",
    "        # Mise à jour de Q_t selon la formule DCC\n",
    "        Q_t = (1 - alpha_2 - beta_2) * Q_bar + alpha_2 * np.outer(residuals[t], residuals[t]) + beta_2 * Q_t\n",
    "        \n",
    "        # Régularisation de la diagonale de Q_t pour éviter les valeurs négatives\n",
    "        diag_Q_t = np.diag(Q_t)\n",
    "        diag_Q_t = np.clip(diag_Q_t, epsilon, None)  # Clip des valeurs négatives ou proches de zéro\n",
    "        Q_t = np.diag(diag_Q_t) + (Q_t - np.diag(np.diag(Q_t)))  # Remplacer la diagonale\n",
    "        \n",
    "        # Ajouter un petit terme de régularisation à Q_t\n",
    "        Q_t += epsilon * np.eye(n_assets)\n",
    "        \n",
    "        # Normaliser Q_t pour obtenir R_t (matrice de corrélation dynamique)\n",
    "        D_t = np.diag(1 / np.sqrt(np.diag(Q_t)))\n",
    "        R_t = D_t @ Q_t @ D_t\n",
    "        \n",
    "        # Vérifier les valeurs propres de R_t et corriger si nécessaire\n",
    "        eigenvalues = np.linalg.eigvals(R_t)\n",
    "        if np.any(eigenvalues <= 0):\n",
    "            R_t += epsilon * np.eye(n_assets)\n",
    "        \n",
    "        # Calcul de la log-vraisemblance pour chaque étape t\n",
    "        try:\n",
    "            term1 = np.log(np.linalg.det(R_t))  # Log du déterminant de R_t\n",
    "        except np.linalg.LinAlgError:\n",
    "            # En cas de matrice singulière\n",
    "            return np.inf  # Retourner une grande valeur pour pénaliser l'optimisation\n",
    "        \n",
    "        term2 = residuals[t].T @ np.linalg.pinv(R_t) @ residuals[t]  # epsilon_t' R_t^{-1} epsilon_t\n",
    "        term3 = residuals[t].T @ residuals[t]  # epsilon_t' epsilon_t\n",
    "        log_likelihood += term1 + term2 + term3  # Ajouter les trois termes pour chaque t\n",
    "    \n",
    "    # Retourner la log-vraisemblance négative\n",
    "    return 0.5 * log_likelihood\n",
    "\n",
    "# Step 6: Optimization of alpha_2 and beta_2\n",
    "def optimize_dcc_garch(returns):\n",
    "    \"\"\"\n",
    "    Optimise les paramètres alpha_2 et beta_2 du modèle DCC-GARCH en utilisant une fenêtre de données.\n",
    "    \n",
    "    Paramètres:\n",
    "    returns (pd.DataFrame): DataFrame des rendements journaliers des actifs (chaque colonne représente un actif).\n",
    "    window (str): Fenêtre temporelle pour la sélection des données ('6M' pour 6 mois, '1Y' pour 1 an, etc.).\n",
    "    \n",
    "    Retourne:\n",
    "    tuple: Paramètres optimisés alpha_2 et beta_2.\n",
    "    \"\"\"\n",
    "    n_assets = returns.shape[1]\n",
    "    \n",
    "    # Ajuster GARCH(1,1) sur chaque actif et standardiser les résidus\n",
    "    volatilities = np.zeros_like(returns)\n",
    "    residuals = np.zeros_like(returns)\n",
    "    \n",
    "    for i in range(n_assets):\n",
    "        volatilities[:, i], residuals[:, i] = fit_garch(returns.iloc[:, i])\n",
    "    \n",
    "    standardized_residuals = residuals / volatilities\n",
    "    \n",
    "    # Valeurs initiales pour alpha_2 et beta_2\n",
    "    initial_params = np.array([0.05, 0.9])\n",
    "    \n",
    "    # Contraintes sur les paramètres\n",
    "    bounds = [(0, 1), (0, 0.95)]  # Limiter beta_2 à 0.95 pour éviter une convergence lente\n",
    "    \n",
    "    # Minimiser la log-vraisemblance négative\n",
    "    result = minimize(dcc_garch_log_likelihood, initial_params, args=(standardized_residuals), bounds=bounds, method='SLSQP', options={'disp': True})\n",
    "    \n",
    "    if result.success:\n",
    "        optimized_params = result.x\n",
    "        return optimized_params\n",
    "    else:\n",
    "        raise ValueError(\"Optimization failed\")\n",
    "\n",
    "def predict_covariance_dcc_garch(df, date, window, horizon=1):\n",
    "    \"\"\"\n",
    "    Calcule la matrice de covariance prédite par le modèle DCC-GARCH à un horizon donné.\n",
    "    \n",
    "    Paramètres:\n",
    "    returns (pd.DataFrame): DataFrame des rendements journaliers des actifs (chaque colonne représente un actif).\n",
    "    window (str): Fenêtre temporelle pour la sélection des données ('6M' pour 6 mois, '1Y' pour 1 an, etc.).\n",
    "    horizon (int): Nombre de jours dans le futur pour lesquels la covariance doit être prédite.\n",
    "    \n",
    "    Retourne:\n",
    "    pd.DataFrame: Matrice de covariance prédite à l'horizon spécifié.\n",
    "    \"\"\"\n",
    "    returns=df.iloc[date-window:date,:]\n",
    "    n_assets = returns.shape[1]\n",
    "    \n",
    "    # Ajuster GARCH(1,1) sur chaque actif et standardiser les résidus\n",
    "    volatilities = np.zeros_like(returns)\n",
    "    residuals = np.zeros_like(returns)\n",
    "    \n",
    "    for i in range(n_assets):\n",
    "        volatilities[:, i], residuals[:, i] = fit_garch(returns.iloc[:, i])\n",
    "    \n",
    "    standardized_residuals = residuals / volatilities\n",
    "    \n",
    "    # Initialiser les paramètres DCC-GARCH avec les valeurs optimisées\n",
    "    optimized_alpha2, optimized_beta2 = optimize_dcc_garch(returns)\n",
    "    \n",
    "    # Initialisation de Q_bar et Q_t\n",
    "    Q_bar = np.cov(standardized_residuals.T)\n",
    "    Q_t = Q_bar.copy()\n",
    "    T = standardized_residuals.shape[0]\n",
    "    \n",
    "    # Utiliser les paramètres optimisés pour prédire la covariance\n",
    "    for t in range(T):\n",
    "        Q_t = (1 - optimized_alpha2 - optimized_beta2) * Q_bar + optimized_alpha2 * np.outer(standardized_residuals[t], standardized_residuals[t]) + optimized_beta2 * Q_t\n",
    "    \n",
    "    # Normaliser Q_t pour obtenir la matrice de corrélation dynamique R_t à la dernière date\n",
    "    D_t = np.diag(1 / np.sqrt(np.diag(Q_t)))\n",
    "    R_t = D_t @ Q_t @ D_t\n",
    "    \n",
    "    # Prédire la matrice de covariance à horizon k jours\n",
    "    last_volatilities = volatilities[-1, :]\n",
    "    D_last = np.diag(last_volatilities)\n",
    "    predicted_covariance = D_last @ R_t @ D_last\n",
    "    \n",
    "    # Boucle pour étendre la prédiction à horizon k jours\n",
    "    for _ in range(horizon):\n",
    "        # Mettre à jour Q_t pour chaque jour à venir\n",
    "        Q_t = (1 - optimized_alpha2 - optimized_beta2) * Q_bar + optimized_beta2 * Q_t\n",
    "        \n",
    "        # Normaliser Q_t pour obtenir R_t (corrélation dynamique après k jours)\n",
    "        D_t = np.diag(1 / np.sqrt(np.diag(Q_t)))\n",
    "        R_t = D_t @ Q_t @ D_t\n",
    "        \n",
    "        # Mise à jour des volatilités pour chaque jour\n",
    "        predicted_covariance = D_last @ R_t @ D_last\n",
    "    \n",
    "    # Retourner la matrice de covariance sous forme de DataFrame avec les colonnes et index des actifs\n",
    "    return pd.DataFrame(predicted_covariance*250, index=returns.columns, columns=returns.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reg_RW_vols(df, date, windows):\n",
    "    Har_vols = []\n",
    "    for actif in df.columns:\n",
    "        vols=[[] for i in range(len(windows))]\n",
    "        vol_realized = []\n",
    "        M=max(windows)\n",
    "        for t in range(date-250, date - M):\n",
    "            for i, window in enumerate(windows):\n",
    "                if window>1:\n",
    "                    vols[i].append(df.loc[t - window+1:t, actif].std() * np.sqrt(250))\n",
    "                else:\n",
    "                    vols[i].append(np.sqrt(df.loc[t , actif]**2 * 250))\n",
    "            vol_realized.append(np.sqrt(df.loc[t+1 , actif]**2 * 250))\n",
    "        vol_data = pd.DataFrame({f'vol_{i+1}': vols[i] for i in range(len(windows))})\n",
    "        vol_data['vol_realized'] = vol_realized\n",
    "        vol_data = vol_data.dropna()\n",
    "        \n",
    "        X = vol_data[[f'vol_{i+1}' for i in range(len(windows))]]\n",
    "        X = add_constant(X)\n",
    "        y = vol_data['vol_realized']\n",
    "        model = OLS(y, X).fit()\n",
    "        latest_vols={}\n",
    "        latest_vols['intercept'] = [1]\n",
    "        for i, window in enumerate(windows):\n",
    "            if window>1:\n",
    "                latest_vols[f'vol_{i+1}']=[df.loc[date - window+1:date, actif].std() * np.sqrt(250)] \n",
    "            else:\n",
    "                latest_vols[f'vol_{i+1}']=[np.sqrt(df.loc[date, actif]**2 * np.sqrt(250))] \n",
    "        \n",
    "        latest_data = pd.DataFrame(latest_vols)\n",
    "        vol_forecast = model.predict(latest_data)\n",
    "        Har_vols.append(vol_forecast.iloc[0])\n",
    "    \n",
    "    return (np.array(Har_vols))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reg_model_vols(df, date, windows):\n",
    "    def ewma_vol(df, date, actif, beta):\n",
    "        vol=0\n",
    "        for t in range(date-100+1,date+1):\n",
    "            vol=beta*vol+(1-beta)*df.loc[t, actif]**2\n",
    "        return(np.sqrt(vol*250))\n",
    "    def ewma_ret(df, date, actif, beta):\n",
    "        ret=0\n",
    "        for t in range(date-50+1,date+1):\n",
    "            ret=beta*ret+(1-beta)*df.loc[t, actif]\n",
    "        return(ret*250)\n",
    "    Har_vols = []\n",
    "    for actif in df.columns:\n",
    "        vols = [[] for i in range(len(windows))]\n",
    "        vol_realized = []\n",
    "        ewma_vols = []\n",
    "        ewma_returns = []\n",
    "        neg_vols = []\n",
    "        avg_vols = []\n",
    "        M = max(windows)\n",
    "        \n",
    "        # Boucle sur la période pour construire les variables\n",
    "        for t in range(date - 250, date - M):\n",
    "            for i, window in enumerate(windows):\n",
    "                if window>1:\n",
    "                    vols[i].append(df.loc[t - window+1:t, actif].std() * np.sqrt(250))\n",
    "                else:\n",
    "                    vols[i].append(np.sqrt(df.loc[t , actif]**2 * 250))\n",
    "            vol_realized.append(np.sqrt(df.loc[t+1 , actif]**2 * 250))\n",
    "\n",
    "            ewma_vols.append(ewma_vol(df, t, actif, 0.94))\n",
    "            ewma_returns.append(ewma_ret(df, t, actif, 0.94))\n",
    "\n",
    "            # Calcul de la volatilité des rendements négatifs sur 5 jours\n",
    "            neg_returns = df.loc[t - 4:t, actif][df.loc[t - 4:t, actif] < 0]\n",
    "            if len(neg_returns) == 0:\n",
    "                neg_vols.append(0)\n",
    "            elif len(neg_returns) == 1:\n",
    "                neg_vols.append(np.sqrt(neg_returns.iloc[0] ** 2 * 250))\n",
    "            else:\n",
    "                neg_vols.append(neg_returns.std() * np.sqrt(250))\n",
    "\n",
    "            # Calcul de la volatilité moyenne sur 5 jours pour tous les actifs\n",
    "            avg_vols.append(df.loc[t - 4:t].std(axis=1).mean() * np.sqrt(250))\n",
    "\n",
    "        # Mettre les données en DataFrame\n",
    "        vol_data = pd.DataFrame({f'vol_{i+1}': vols[i] for i in range(len(windows))})\n",
    "        vol_data['vol_realized'] = vol_realized\n",
    "        vol_data['ewma_vol'] = ewma_vols\n",
    "        vol_data['ewma_return'] = ewma_returns\n",
    "        vol_data['neg_vol'] = neg_vols\n",
    "        vol_data['avg_vol'] = avg_vols\n",
    "        vol_data = vol_data.dropna()\n",
    "\n",
    "        # Régression linéaire\n",
    "        X = vol_data[[f'vol_{i+1}' for i in range(len(windows))] + ['ewma_vol', 'ewma_return', 'neg_vol', 'avg_vol']]\n",
    "        X = add_constant(X)\n",
    "        y = vol_data['vol_realized']\n",
    "        model = OLS(y, X).fit()\n",
    "\n",
    "        # Prédiction des volatilités futures\n",
    "        latest_vols = {'intercept': [1]}\n",
    "        for i, window in enumerate(windows):\n",
    "            if window > 1:\n",
    "                latest_vols[f'vol_{i+1}'] = [df.loc[date - window + 1:date, actif].std() * np.sqrt(250)]\n",
    "            else:\n",
    "                latest_vols[f'vol_{i+1}'] = [np.sqrt(df.loc[date, actif] ** 2 * 250)]\n",
    "        \n",
    "        latest_vols['ewma_vol'] = [ewma_vol(df, date, actif, 0.94)]\n",
    "        latest_vols['ewma_return'] = [ewma_ret(df, date, actif, 0.94)]\n",
    "        \n",
    "        neg_returns = df.loc[date - 4:date, actif][df.loc[date - 4:date, actif] < 0]\n",
    "        if len(neg_returns) == 0:\n",
    "            latest_vols['neg_vol'] = [0]\n",
    "        elif len(neg_returns) == 1:\n",
    "            latest_vols['neg_vol'] = [np.sqrt(neg_returns.iloc[0] ** 2)* 250]\n",
    "        else:\n",
    "            latest_vols['neg_vol'] = [neg_returns.std() * np.sqrt(250)]\n",
    "        \n",
    "        latest_vols['avg_vol'] = [df.loc[date - 4:date].std(axis=1).mean() * np.sqrt(250)]\n",
    "        \n",
    "        lastest_data = pd.DataFrame(latest_vols)\n",
    "        vol_forecast = model.predict(lastest_data)\n",
    "        Har_vols.append(vol_forecast.iloc[0])\n",
    "    \n",
    "    return np.array(Har_vols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.27234684, 0.25998363, 0.14852436, 0.13697378, 0.29743358])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reg_RW_vols(returns.iloc[:,:5], 1000, [1,5,21])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.30784448, 0.33990836, 0.1609533 , 0.14720231, 0.28712086])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reg_model_vols(returns.iloc[:,:5],  1000, [1,5,21])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\33640\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\core\\internals\\blocks.py:393: RuntimeWarning: invalid value encountered in sqrt\n",
      "  result = func(self.values, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>A</th>\n",
       "      <th>AAPL</th>\n",
       "      <th>ABT</th>\n",
       "      <th>ACGL</th>\n",
       "      <th>ADBE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>A</th>\n",
       "      <td>0.269033</td>\n",
       "      <td>0.239699</td>\n",
       "      <td>0.074348</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.143816</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AAPL</th>\n",
       "      <td>0.239699</td>\n",
       "      <td>0.282170</td>\n",
       "      <td>0.058820</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.215717</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ABT</th>\n",
       "      <td>0.074348</td>\n",
       "      <td>0.058820</td>\n",
       "      <td>0.113243</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.108515</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ACGL</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.141759</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ADBE</th>\n",
       "      <td>0.143816</td>\n",
       "      <td>0.215717</td>\n",
       "      <td>0.108515</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.209910</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             A      AAPL       ABT      ACGL      ADBE\n",
       "A     0.269033  0.239699  0.074348       NaN  0.143816\n",
       "AAPL  0.239699  0.282170  0.058820       NaN  0.215717\n",
       "ABT   0.074348  0.058820  0.113243       NaN  0.108515\n",
       "ACGL       NaN       NaN       NaN  0.141759       NaN\n",
       "ADBE  0.143816  0.215717  0.108515       NaN  0.209910"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sqrt(returns.iloc[1000:1005,:5].cov()*250)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Cov_Models:\n",
    "    def __init__(self, df, annualization):\n",
    "        self.df = df\n",
    "        self.annualization = annualization \n",
    "    def histo_cov(self, window):\n",
    "        if window == 1:\n",
    "            # Approxime la covariance en utilisant le produit des rendements pour chaque paire d'actifs\n",
    "            cov_matrix = pd.DataFrame(index=self.df.columns, columns=self.df.columns)\n",
    "            for col1 in self.df.columns:\n",
    "                for col2 in self.df.columns:\n",
    "                    cov_matrix.loc[col1, col2] = self.df.iloc[-1][col1] * self.df.iloc[-1][col2] * self.annualization\n",
    "        else:\n",
    "            # Calcul standard de la covariance sur la fenêtre définie\n",
    "            cov_matrix = self.df.iloc[len(self.df.index) - window:len(self.df.index), :].cov() * self.annualization\n",
    "\n",
    "        return cov_matrix\n",
    "    \n",
    "    def EWMA_series(self,beta, actif1,actif2):\n",
    "        ewma_cov=0\n",
    "        m1 = max(self.df.loc[:, actif1].first_valid_index(), self.df.loc[:, actif2].first_valid_index())\n",
    "        while self.df.loc[m1:, actif1].isna().any() or self.df.loc[m1:, actif2].isna().any():\n",
    "            m1+=1\n",
    "        for t in range(m1,len(self.df.index)):\n",
    "            r_t1 = self.df.loc[t,actif1]\n",
    "            r_t2 = self.df.loc[t,actif2]\n",
    "            ewma_cov=beta*ewma_cov+(1-beta)* r_t1*r_t2\n",
    "        ewma_cov=ewma_cov*self.annualization\n",
    "        return (ewma_cov)\n",
    "    def EWMA_cov_opti(self, actif1,actif2, horizon):\n",
    "        betas = np.linspace(0.90, 0.99, 21)\n",
    "        covs = [[] for _ in range(len(betas))]\n",
    "        score= [[] for _ in range(len(betas))]\n",
    "        for j, beta in enumerate(betas):\n",
    "            cov_realized = []\n",
    "            for t in range(len(self.df.index)-100-horizon,len(self.df.index)-horizon, 10):\n",
    "                realized=0\n",
    "                for j in range(horizon):\n",
    "                    realized+= self.df.loc[t+j:t + j, actif1]* self.df.loc[t+j:t + j, actif2] * self.annualization\n",
    "                cov_realized.append(realized)\n",
    "                covs[j].append(self.EWMA_series(beta, actif1,actif2))\n",
    "            score[j]=np.sum((np.array(cov_realized)-np.array(covs[j]))**2)\n",
    "        beta_opt=betas[score.index(min(score))]\n",
    "        return(beta_opt)\n",
    "    def EWMA_cov(self, beta_var, beta_cov, horizon):\n",
    "        ewma_cov=[]\n",
    "        pairs=[]\n",
    "        for j in range(len(self.df.columns)):\n",
    "            for i in range(j, len(self.df.columns)):\n",
    "                pairs.append((self.df.columns[j], self.df.columns[i]))\n",
    "        for j, (actif1, actif2) in enumerate (pairs):\n",
    "            if beta_var=='Opti' and beta_cov=='Opti':\n",
    "                ewma_cov.append(self.EWMA_series(self.EWMA_cov_opti( actif1,actif2, horizon), actif1,actif2))\n",
    "            else:\n",
    "                if actif1==actif2:\n",
    "                    ewma_cov.append(self.EWMA_series(beta_var, actif1,actif2))\n",
    "                else:\n",
    "                    ewma_cov.append(self.EWMA_series(beta_cov, actif1,actif2))\n",
    "        covs=np.zeros((len(self.df.columns),len(self.df.columns)))\n",
    "        k=0\n",
    "        for j in range(len(self.df.columns)):\n",
    "            for i in range(j, len(self.df.columns)):\n",
    "                covs[j,i]=covs[i,j]=ewma_cov[k]\n",
    "                k+=1\n",
    "        covariance=pd.DataFrame(covs, columns=self.df.columns, index=self.df.columns)\n",
    "        return covariance\n",
    "    \n",
    "    def HAR_cov(self, windows_var, windows_cov, horizon):\n",
    "        Har_covs = []\n",
    "        pairs = []\n",
    "        for j in range(len(self.df.columns)):\n",
    "            for i in range(j, len(self.df.columns)):\n",
    "                pairs.append((self.df.columns[j], self.df.columns[i]))\n",
    "        for actif1, actif2 in pairs:\n",
    "            m1 = max(self.df.loc[:, actif1].first_valid_index(), self.df.loc[:, actif2].first_valid_index())\n",
    "            while self.df.loc[m1:, actif1].isna().any() or self.df.loc[m1:, actif2].isna().any():\n",
    "                m1 += 1\n",
    "            if actif1 == actif2:\n",
    "                windows = windows_var\n",
    "            else:\n",
    "                windows = windows_cov\n",
    "            covs = [[] for _ in range(len(windows))]\n",
    "            cov_realized = []\n",
    "            for t in range(max(max(windows[-1], m1),len(self.df.index) - 250),  len(self.df.index) - horizon):\n",
    "                for i, window in enumerate(windows):\n",
    "                    if window > 1:\n",
    "                        cov_value = self.df.loc[t - window:t - 1, [actif1, actif2]].cov().iloc[0, 1] * self.annualization\n",
    "                    else:\n",
    "                        # Calcul de covariance pour une fenêtre de taille 1\n",
    "                        cov_value = self.df.loc[t - 1, actif1] * self.df.loc[t - 1, actif2] * self.annualization\n",
    "                    covs[i].append(cov_value)\n",
    "\n",
    "                # Calcul pour cov_realized\n",
    "                if horizon > 1:\n",
    "                    cov_realized_value = self.df.loc[t:t + horizon, [actif1, actif2]].cov().iloc[0, 1] * self.annualization\n",
    "                else:\n",
    "                    cov_realized_value = self.df.loc[t, actif1] * self.df.loc[t, actif2] * self.annualization\n",
    "                cov_realized.append(cov_realized_value)\n",
    "\n",
    "            cov_data = pd.DataFrame({f'cov_{i+1}': covs[i] for i in range(len(windows))})\n",
    "            cov_data['cov_realized'] = cov_realized\n",
    "            cov_data = cov_data.dropna()\n",
    "\n",
    "            X = cov_data[[f'cov_{i+1}' for i in range(len(windows))]]\n",
    "            X = add_constant(X)\n",
    "            y = cov_data['cov_realized']\n",
    "            model = OLS(y, X).fit()\n",
    "            latest_covs = {'intercept': [1]}\n",
    "            for i, window in enumerate(windows):\n",
    "                if window > 1:\n",
    "                    latest_cov_value = self.df.loc[len(self.df.index) - window:len(self.df.index) - 1, [actif1, actif2]].cov().iloc[0, 1] * self.annualization\n",
    "                else:\n",
    "                    latest_cov_value = self.df.loc[len(self.df.index) - 1, actif1] * self.df.loc[len(self.df.index) - 1, actif2] * self.annualization\n",
    "                latest_covs[f'cov_{i+1}'] = [latest_cov_value]\n",
    "\n",
    "            lastest_data = pd.DataFrame(latest_covs)\n",
    "            cov_forecast = model.predict(lastest_data)\n",
    "            Har_covs.append(cov_forecast.iloc[0])\n",
    "            if progress_bar:  # Update the progress bar if provided\n",
    "                progress_bar.update(1)\n",
    "        covs = np.zeros((len(self.df.columns), len(self.df.columns)))\n",
    "        k = 0\n",
    "        for j in range(len(self.df.columns)):\n",
    "            for i in range(j, len(self.df.columns)):\n",
    "                covs[j, i] = covs[i, j] = Har_covs[k]\n",
    "                k += 1\n",
    "        covariance = pd.DataFrame(covs, columns=self.df.columns, index=self.df.columns)\n",
    "        return covariance\n",
    "        \n",
    "    def timeseries_to_supervised(self, data, lag=1):\n",
    "        supervised_data = pd.DataFrame(data)\n",
    "        columns = [supervised_data.shift(i) for i in range(1, lag+1)]\n",
    "        columns.append(supervised_data)\n",
    "        supervised_data = pd.concat(columns, axis=1)\n",
    "        supervised_data.fillna(0, inplace=True)\n",
    "        supervised_data = supervised_data[2:].reset_index(drop=True)\n",
    "        return supervised_data\n",
    "\n",
    "    def fit_lstm(self, X, y, batch_size, nb_epoch, neurons):\n",
    "        # Define the model with the preferred Input(shape) object\n",
    "        model = Sequential()\n",
    "        model.add(Input(shape=(X.shape[1], X.shape[2])))  # Using Input layer to define input shape\n",
    "        model.add(LSTM(neurons))\n",
    "        model.add(Dropout(0.2))\n",
    "        model.add(Dense(1))\n",
    "\n",
    "        # Compile the model\n",
    "        model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "\n",
    "        # Fit the model\n",
    "        for _ in range(nb_epoch):\n",
    "            model.fit(X, y, epochs=1, batch_size=batch_size, verbose=0, shuffle=False)\n",
    "\n",
    "        return model\n",
    "\n",
    "    def LSTM_cov(self,  batch_size, nb_epoch, neurons, horizon):\n",
    "        cov = []\n",
    "        pairs=[]\n",
    "        Stationnary={}\n",
    "        for actif in self.df.columns:\n",
    "            rendements=self.df.loc[:,actif]\n",
    "            m1=rendements.first_valid_index()\n",
    "            while rendements.iloc[m1:].isna().any():\n",
    "                m1+=1\n",
    "            y=rendements.iloc[m1:]\n",
    "            adf_result = adfuller(y)\n",
    "            if adf_result[1]>=0.05:\n",
    "                Stationnary[actif]=False\n",
    "            else:\n",
    "                Stationnary[actif]=True\n",
    "        k=0\n",
    "        for j in range(len(self.df.columns)):\n",
    "            for i in range(j, len(self.df.columns)):\n",
    "                k+=1\n",
    "                pairs.append((self.df.columns[j], self.df.columns[i]))\n",
    "        k=0\n",
    "        for actif1, actif2 in pairs:\n",
    "            if Stationnary[actif1]==False or Stationnary[actif2]==False:\n",
    "                cov.append(self.df.loc[len(self.df.index)-horizon*self.annualization:len(self.df.index), [actif1,actif2]].cov().iloc[0,1] * self.annualization)\n",
    "            else:\n",
    "                m1 = max(self.df.loc[:, actif1].first_valid_index(), self.df.loc[:, actif2].first_valid_index())\n",
    "                while self.df.loc[m1:, actif1].isna().any() or self.df.loc[m1:, actif2].isna().any():\n",
    "                    m1+=1\n",
    "                constante1 = self.df.loc[0:len(self.df.index)-96, [actif1,actif2]].cov().iloc[0,1] * self.annualization\n",
    "                constante2 = self.df.loc[len(self.df.index)-96:len(self.df.index), [actif1,actif2]].cov().iloc[0,1] * self.annualization\n",
    "                high = max(constante1, constante2)\n",
    "                low = min(constante1, constante2)\n",
    "\n",
    "                if horizon == 1:\n",
    "                    if (high - low) / low >= 0.3:\n",
    "                        X = np.array([self.df.loc[t-5:t, [actif1,actif2]].cov().iloc[0,1] * self.annualization for t in range(max(len(self.df.index)-108, m1), len(self.df.index), 6)])\n",
    "                    else:\n",
    "                        X = np.array([self.df.loc[t-5:t, [actif1,actif2]].cov().iloc[0,1] * self.annualization for t in range(max(m1, 5), len(self.df.index), 6)])\n",
    "                        ind_to_remove = np.argsort(X)[-4:]\n",
    "                        mask = np.ones(len(X), dtype=bool)\n",
    "                        mask[ind_to_remove] = False\n",
    "                        X = X[mask]\n",
    "                elif horizon == 3:\n",
    "                    if (high - low) / low >= 0.3:\n",
    "                        X = np.array([self.df.loc[t-5:t, [actif1,actif2]].cov().iloc[0,1] * self.annualization for t in range(max(len(self.df.index)-144, m1), len(self.df.index), 6)])\n",
    "                    else:\n",
    "                        X = np.array([self.df.loc[t-5:t, [actif1,actif2]].cov().iloc[0,1] * self.annualization for t in range(max(m1, 5), len(self.df.index), 6)])\n",
    "                        ind_to_remove = np.argsort(X)[-3:]\n",
    "                        mask = np.ones(len(X), dtype=bool)\n",
    "                        mask[ind_to_remove] = False\n",
    "                        X = X[mask]\n",
    "                elif horizon == 5 or horizon == 7:\n",
    "                    if (high - low) / low >= 0.3:\n",
    "                        X = np.array([self.df.loc[t-5:t, [actif1,actif2]].cov().iloc[0,1] * self.annualization for t in range(max(len(self.df.index)-180, m1), len(self.df.index), 6)])\n",
    "                    else:\n",
    "                        X = np.array([self.df.loc[t-5:t, [actif1,actif2]].cov().iloc[0,1] * self.annualization for t in range(max(m1, 5), len(self.df.index), 6)])\n",
    "                        ind_to_remove = np.argsort(X)[-2:]\n",
    "                        mask = np.ones(len(X), dtype=bool)\n",
    "                        mask[ind_to_remove] = False\n",
    "                        X = X[mask]\n",
    "                elif horizon == 10:\n",
    "                    if (high - low) / low >= 0.3:\n",
    "                        X = np.array([self.df.loc[t-5:t, [actif1,actif2]].cov().iloc[0,1] * self.annualization for t in range(max(len(self.df.index)-240, m1), len(self.df.index), 6)])\n",
    "                    else:\n",
    "                        X = np.array([self.df.loc[t-5:t, [actif1,actif2]].cov().iloc[0,1] * self.annualization for t in range(max(m1, 5), len(self.df.index), 6)])\n",
    "                        ind_to_remove = np.argsort(X)[-1:]\n",
    "                        mask = np.ones(len(X), dtype=bool)\n",
    "                        mask[ind_to_remove] = False\n",
    "                        X = X[mask]\n",
    "\n",
    "                X = X.reshape(len(X), 1)\n",
    "                scaler = MinMaxScaler(feature_range=(-1, 1))\n",
    "                scaler = scaler.fit(X)\n",
    "                scaled_X = scaler.transform(X)\n",
    "                train = self.timeseries_to_supervised(scaled_X, lag=1)\n",
    "                values = train.values\n",
    "                X, y = values[:, 0].reshape(-1, 1), values[:, 1].reshape(-1, 1)\n",
    "                X = X.reshape((X.shape[0], 1, X.shape[1]))\n",
    "\n",
    "                model = self.fit_lstm(X, y, batch_size, nb_epoch, neurons)\n",
    "                cov_forecast = model.predict(X, batch_size=1)\n",
    "                cov.append(scaler.inverse_transform(cov_forecast)[:2*horizon, 0].mean())\n",
    "                k+=1\n",
    "                if progress_bar:  # Update the progress bar if provided\n",
    "                    progress_bar.update(1)\n",
    "        covs=np.zeros((len(self.df.columns),len(self.df.columns)))\n",
    "        k=0\n",
    "        for j in range(len(self.df.columns)):\n",
    "            for i in range(j, len(self.df.columns)):\n",
    "                covs[j,i]=covs[i,j]=cov[k]\n",
    "                k+=1\n",
    "                \n",
    "        covariance=pd.DataFrame(covs, columns=self.df.columns, index=self.df.columns)\n",
    "        volatilities = np.sqrt(np.diag(covariance))\n",
    "\n",
    "        # Parcourir chaque élément de la matrice\n",
    "        for i in range(covariance.shape[0]):\n",
    "            for j in range(covariance.shape[1]):\n",
    "                # Calculer le produit des volatilités correspondantes\n",
    "                vol_product = volatilities[i] * volatilities[j]\n",
    "                # Vérifier si la covariance est supérieure au produit des volatilités\n",
    "                if covariance.iloc[i, j] > vol_product:\n",
    "                    # Remplacer la valeur par le produit des volatilités\n",
    "                    covariance.iloc[i, j] = vol_product\n",
    "        return covariance\n",
    "\n",
    "    def volatilite(self, covariance):\n",
    "        vol=[]\n",
    "        for i in range(covariance.shape[0]):\n",
    "            vol.append(np.sqrt(covariance.iloc[i,i]))\n",
    "        return(np.array(vol))\n",
    "    \n",
    "    def correlation(self, covariance):\n",
    "        correl= np.zeros((covariance.shape[0],covariance.shape[0]))\n",
    "        vol=self.volatilite(covariance)\n",
    "        for i in range(covariance.shape[0]):\n",
    "            for j in range(covariance.shape[1]):\n",
    "                correl[i,j]=covariance.iloc[i,j]/(vol[i]*vol[j])\n",
    "        return(pd.DataFrame(correl, columns=self.df.columns, index=self.df.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
