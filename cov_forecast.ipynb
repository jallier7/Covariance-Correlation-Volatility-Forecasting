{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "import math\n",
    "import seaborn as sns\n",
    "from cvx.covariance.combination import from_ewmas\n",
    "from arch import arch_model\n",
    "from statsmodels.tsa.vector_ar.vecm import coint_johansen\n",
    "from scipy.optimize import minimize\n",
    "from statsmodels.tools.tools import add_constant\n",
    "from statsmodels.regression.linear_model import OLS\n",
    "from sklearn.linear_model import Ridge, Lasso\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import r2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv(r'C:\\Users\\33640\\OneDrive\\Documents\\GitHub\\Covariance-Correlation-Volatility-Forecasting\\SP500_daily_returns.csv')\n",
    "nan_counts=df.isna().sum()\n",
    "columns_to_keep = nan_counts[nan_counts <= 3].index\n",
    "returns = df[columns_to_keep]\n",
    "returns=returns.dropna()\n",
    "returns.shape\n",
    "dates=returns['Date'] \n",
    "returns=returns.drop('Date', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def histo_cov(df, date, window):\n",
    "    cov=np.zeros((len(df.columns),len(df.columns)))\n",
    "    for i in range(date-window+1, date+1):\n",
    "        cov+=np.outer(np.array(df.iloc[i,:]),np.array(df.iloc[i,:]))/(window)\n",
    "    covs=pd.DataFrame(cov*250, columns=df.columns, index=df.columns)\n",
    "    return(covs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "halflife_pairs=[(10, 21), (21, 63), (63, 125)]\n",
    "def ewma_cov(df, date, halflife_pairs, window=500):    \n",
    "    combinator = from_ewmas(df.iloc[max(0,date-1000):date,:],\n",
    "                                halflife_pairs,\n",
    "                                min_periods_vola=window,  # min periods for volatility estimation\n",
    "                                min_periods_cov=window)  # min periods for correlation estimation\n",
    "\n",
    "    # Solve combination problem and loop through combination results to get predictors\n",
    "    covariance_predictors = {}\n",
    "    for predictor in combinator.solve(window=10):  # lookback window for optimization\n",
    "        # From predictor we can access predictor.time, predictor.mean (=0 here),\n",
    "        # predictor.covariance, and predictor.weights\n",
    "        covariance_predictors[predictor.time] = predictor.covariance\n",
    "    return(covariance_predictors[date]*250)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Fit GARCH(1,1) to each asset's returns\n",
    "def fit_garch(returns):\n",
    "    # Rescaler les rendements\n",
    "    returns_rescaled = returns * 100\n",
    "    \n",
    "    model = arch_model(returns_rescaled, vol='Garch', p=1, q=1)\n",
    "    garch_fit = model.fit(disp=\"off\")\n",
    "    \n",
    "    # Revenir à l'échelle d'origine pour les résidus\n",
    "    residuals = garch_fit.resid / 100\n",
    "    volatilities = garch_fit.conditional_volatility / 100\n",
    "    return volatilities, residuals\n",
    "# Step 2: DCC-GARCH log-likelihood function\n",
    "def dcc_garch_log_likelihood(params, residuals):\n",
    "    \"\"\"\n",
    "    Calcule la log-vraisemblance du modèle DCC-GARCH.\n",
    "    \n",
    "    Paramètres:\n",
    "    params (tuple): Paramètres alpha_2 et beta_2.\n",
    "    residuals (np.array): Matrice des résidus standardisés (T x N).\n",
    "    \n",
    "    Retourne:\n",
    "    float: La valeur de la log-vraisemblance négative.\n",
    "    \"\"\"\n",
    "    alpha_2, beta_2 = params\n",
    "    n_assets = residuals.shape[1]\n",
    "    \n",
    "    # Initialisation de Q_bar et Q_t\n",
    "    T = residuals.shape[0]\n",
    "    Q_bar = np.cov(residuals.T)  # Matrice de corrélation inconditionnelle\n",
    "    Q_t = Q_bar.copy()\n",
    "    \n",
    "    log_likelihood = 0\n",
    "    epsilon = 1e-5  # Petit terme de régularisation pour éviter les matrices singulières\n",
    "    \n",
    "    for t in range(T):\n",
    "        # Mise à jour de Q_t selon la formule DCC\n",
    "        Q_t = (1 - alpha_2 - beta_2) * Q_bar + alpha_2 * np.outer(residuals[t], residuals[t]) + beta_2 * Q_t\n",
    "        \n",
    "        # Régularisation de la diagonale de Q_t pour éviter les valeurs négatives\n",
    "        diag_Q_t = np.diag(Q_t)\n",
    "        diag_Q_t = np.clip(diag_Q_t, epsilon, None)  # Clip des valeurs négatives ou proches de zéro\n",
    "        Q_t = np.diag(diag_Q_t) + (Q_t - np.diag(np.diag(Q_t)))  # Remplacer la diagonale\n",
    "        \n",
    "        # Ajouter un petit terme de régularisation à Q_t\n",
    "        Q_t += epsilon * np.eye(n_assets)\n",
    "        \n",
    "        # Normaliser Q_t pour obtenir R_t (matrice de corrélation dynamique)\n",
    "        D_t = np.diag(1 / np.sqrt(np.diag(Q_t)))\n",
    "        R_t = D_t @ Q_t @ D_t\n",
    "        \n",
    "        # Vérifier les valeurs propres de R_t et corriger si nécessaire\n",
    "        eigenvalues = np.linalg.eigvals(R_t)\n",
    "        if np.any(eigenvalues <= 0):\n",
    "            R_t += epsilon * np.eye(n_assets)\n",
    "        \n",
    "        # Calcul de la log-vraisemblance pour chaque étape t\n",
    "        try:\n",
    "            term1 = np.log(np.linalg.det(R_t))  # Log du déterminant de R_t\n",
    "        except np.linalg.LinAlgError:\n",
    "            # En cas de matrice singulière\n",
    "            return np.inf  # Retourner une grande valeur pour pénaliser l'optimisation\n",
    "        \n",
    "        term2 = residuals[t].T @ np.linalg.pinv(R_t) @ residuals[t]  # epsilon_t' R_t^{-1} epsilon_t\n",
    "        term3 = residuals[t].T @ residuals[t]  # epsilon_t' epsilon_t\n",
    "        log_likelihood += term1 + term2 + term3  # Ajouter les trois termes pour chaque t\n",
    "    \n",
    "    # Retourner la log-vraisemblance négative\n",
    "    return 0.5 * log_likelihood\n",
    "\n",
    "# Step 6: Optimization of alpha_2 and beta_2\n",
    "def optimize_dcc_garch(returns):\n",
    "    \"\"\"\n",
    "    Optimise les paramètres alpha_2 et beta_2 du modèle DCC-GARCH en utilisant une fenêtre de données.\n",
    "    \n",
    "    Paramètres:\n",
    "    returns (pd.DataFrame): DataFrame des rendements journaliers des actifs (chaque colonne représente un actif).\n",
    "    window (str): Fenêtre temporelle pour la sélection des données ('6M' pour 6 mois, '1Y' pour 1 an, etc.).\n",
    "    \n",
    "    Retourne:\n",
    "    tuple: Paramètres optimisés alpha_2 et beta_2.\n",
    "    \"\"\"\n",
    "    n_assets = returns.shape[1]\n",
    "    \n",
    "    # Ajuster GARCH(1,1) sur chaque actif et standardiser les résidus\n",
    "    volatilities = np.zeros_like(returns)\n",
    "    residuals = np.zeros_like(returns)\n",
    "    \n",
    "    for i in range(n_assets):\n",
    "        volatilities[:, i], residuals[:, i] = fit_garch(returns.iloc[:, i])\n",
    "    \n",
    "    standardized_residuals = residuals / volatilities\n",
    "    \n",
    "    # Valeurs initiales pour alpha_2 et beta_2\n",
    "    initial_params = np.array([0.05, 0.9])\n",
    "    \n",
    "    # Contraintes sur les paramètres\n",
    "    bounds = [(0, 1), (0, 0.95)]  # Limiter beta_2 à 0.95 pour éviter une convergence lente\n",
    "    \n",
    "    # Minimiser la log-vraisemblance négative\n",
    "    result = minimize(dcc_garch_log_likelihood, initial_params, args=(standardized_residuals), bounds=bounds, method='SLSQP', options={'disp': True})\n",
    "    \n",
    "    if result.success:\n",
    "        optimized_params = result.x\n",
    "        return optimized_params\n",
    "    else:\n",
    "        raise ValueError(\"Optimization failed\")\n",
    "\n",
    "def predict_covariance_dcc_garch(df, date, window, horizon=1):\n",
    "    \"\"\"\n",
    "    Calcule la matrice de covariance prédite par le modèle DCC-GARCH à un horizon donné.\n",
    "    \n",
    "    Paramètres:\n",
    "    returns (pd.DataFrame): DataFrame des rendements journaliers des actifs (chaque colonne représente un actif).\n",
    "    window (str): Fenêtre temporelle pour la sélection des données ('6M' pour 6 mois, '1Y' pour 1 an, etc.).\n",
    "    horizon (int): Nombre de jours dans le futur pour lesquels la covariance doit être prédite.\n",
    "    \n",
    "    Retourne:\n",
    "    pd.DataFrame: Matrice de covariance prédite à l'horizon spécifié.\n",
    "    \"\"\"\n",
    "    returns=df.iloc[date-window:date,:]\n",
    "    n_assets = returns.shape[1]\n",
    "    \n",
    "    # Ajuster GARCH(1,1) sur chaque actif et standardiser les résidus\n",
    "    volatilities = np.zeros_like(returns)\n",
    "    residuals = np.zeros_like(returns)\n",
    "    \n",
    "    for i in range(n_assets):\n",
    "        volatilities[:, i], residuals[:, i] = fit_garch(returns.iloc[:, i])\n",
    "    \n",
    "    standardized_residuals = residuals / volatilities\n",
    "    \n",
    "    # Initialiser les paramètres DCC-GARCH avec les valeurs optimisées\n",
    "    optimized_alpha2, optimized_beta2 = optimize_dcc_garch(returns)\n",
    "    \n",
    "    # Initialisation de Q_bar et Q_t\n",
    "    Q_bar = np.cov(standardized_residuals.T)\n",
    "    Q_t = Q_bar.copy()\n",
    "    T = standardized_residuals.shape[0]\n",
    "    \n",
    "    # Utiliser les paramètres optimisés pour prédire la covariance\n",
    "    for t in range(T):\n",
    "        Q_t = (1 - optimized_alpha2 - optimized_beta2) * Q_bar + optimized_alpha2 * np.outer(standardized_residuals[t], standardized_residuals[t]) + optimized_beta2 * Q_t\n",
    "    \n",
    "    # Normaliser Q_t pour obtenir la matrice de corrélation dynamique R_t à la dernière date\n",
    "    D_t = np.diag(1 / np.sqrt(np.diag(Q_t)))\n",
    "    R_t = D_t @ Q_t @ D_t\n",
    "    \n",
    "    # Prédire la matrice de covariance à horizon k jours\n",
    "    last_volatilities = volatilities[-1, :]\n",
    "    D_last = np.diag(last_volatilities)\n",
    "    predicted_covariance = D_last @ R_t @ D_last\n",
    "    \n",
    "    # Boucle pour étendre la prédiction à horizon k jours\n",
    "    for _ in range(horizon):\n",
    "        # Mettre à jour Q_t pour chaque jour à venir\n",
    "        Q_t = (1 - optimized_alpha2 - optimized_beta2) * Q_bar + optimized_beta2 * Q_t\n",
    "        \n",
    "        # Normaliser Q_t pour obtenir R_t (corrélation dynamique après k jours)\n",
    "        D_t = np.diag(1 / np.sqrt(np.diag(Q_t)))\n",
    "        R_t = D_t @ Q_t @ D_t\n",
    "        \n",
    "        # Mise à jour des volatilités pour chaque jour\n",
    "        predicted_covariance = D_last @ R_t @ D_last\n",
    "    \n",
    "    # Retourner la matrice de covariance sous forme de DataFrame avec les colonnes et index des actifs\n",
    "    return pd.DataFrame(predicted_covariance*250, index=returns.columns, columns=returns.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reg_RW_vols(df, date, windows):\n",
    "    Har_vols = []\n",
    "    for actif in df.columns:\n",
    "        vols=[[] for i in range(len(windows))]\n",
    "        vol_realized = []\n",
    "        M=max(windows)\n",
    "        for t in range(date-250, date ):\n",
    "            for i, window in enumerate(windows):\n",
    "                if window>1:\n",
    "                    vols[i].append(df.loc[t - window+1:t, actif].std() * np.sqrt(250))\n",
    "                else:\n",
    "                    vols[i].append(np.sqrt(df.loc[t , actif]**2 * 250))\n",
    "            vol_realized.append(np.sqrt(df.loc[t+1 , actif]**2 * 250))\n",
    "        vol_data = pd.DataFrame({f'vol_{i+1}': vols[i] for i in range(len(windows))})\n",
    "        vol_data['vol_realized'] = vol_realized\n",
    "        vol_data = vol_data.dropna()\n",
    "        \n",
    "        X = vol_data[[f'vol_{i+1}' for i in range(len(windows))]]\n",
    "        X = add_constant(X)\n",
    "        y = vol_data['vol_realized']\n",
    "        model = OLS(y, X).fit()\n",
    "        latest_vols={}\n",
    "        latest_vols['intercept'] = [1]\n",
    "        for i, window in enumerate(windows):\n",
    "            if window>1:\n",
    "                latest_vols[f'vol_{i+1}']=[df.loc[date - window+1:date, actif].std() * np.sqrt(250)] \n",
    "            else:\n",
    "                latest_vols[f'vol_{i+1}']=[np.sqrt(df.loc[date, actif]**2 * np.sqrt(250))] \n",
    "        \n",
    "        latest_data = pd.DataFrame(latest_vols)\n",
    "        vol_forecast = model.predict(latest_data)\n",
    "        Har_vols.append(vol_forecast.iloc[0])\n",
    "    \n",
    "    return (np.array(Har_vols))\n",
    "\n",
    "\n",
    "\n",
    "def ewma_covariance(df_returns, date, beta=0.94):\n",
    "    # Nombre d'actifs (colonnes)\n",
    "    n_assets = df_returns.shape[1]\n",
    "    \n",
    "    # Calcul de la moyenne des rendements\n",
    "    mean_returns = df_returns.mean()\n",
    "    \n",
    "    # Initialisation de la matrice de covariance EWMA\n",
    "    ewma_cov = np.zeros((n_assets, n_assets))\n",
    "    \n",
    "    # Matrice de covariance initiale basée sur les premières données\n",
    "    for t in range(date):\n",
    "        # Déviation des rendements par rapport à la moyenne\n",
    "        diff = (df_returns.iloc[t, :] - mean_returns).values.reshape(-1, 1)\n",
    "        \n",
    "        # Limiter la magnitude des valeurs pour éviter l'overflow\n",
    "        if np.any(np.abs(diff) > 1e5):\n",
    "            diff = np.clip(diff, -1e5, 1e5)\n",
    "        \n",
    "        # Mise à jour de la matrice de covariance EWMA\n",
    "        ewma_cov = (1 - beta) * np.dot(diff, diff.T) + beta * ewma_cov\n",
    "    \n",
    "    # Transformation en DataFrame pour un affichage plus lisible\n",
    "    ewma_cov_df = pd.DataFrame(ewma_cov, index=df_returns.columns, columns=df_returns.columns)\n",
    "    \n",
    "    return ewma_cov_df\n",
    "\n",
    "def ewma_correlation(df_returns, date, beta=0.97):\n",
    "    # Calcul de la matrice de covariance EWMA\n",
    "    ewma_cov_df = ewma_covariance(df_returns, date, beta)\n",
    "    \n",
    "    # Calcul des écarts-types EWMA\n",
    "    ewma_std = np.sqrt(np.diag(ewma_cov_df))\n",
    "    \n",
    "    # Matrice de corrélation EWMA\n",
    "    ewma_corr = ewma_cov_df / np.outer(ewma_std, ewma_std)\n",
    "    \n",
    "    # Transformation en DataFrame\n",
    "    ewma_corr_df = pd.DataFrame(ewma_corr, index=df_returns.columns, columns=df_returns.columns)\n",
    "    \n",
    "    return ewma_corr_df\n",
    "\n",
    "def covariance_matrix(volatility_vector, correlation_matrix):\n",
    "    # volatility_vector: 1D numpy array or list of volatilities (standard deviations)\n",
    "    # correlation_matrix: 2D numpy array or DataFrame of correlations\n",
    "    \n",
    "    # Convert the volatility vector into a diagonal matrix\n",
    "    vol_diag = np.diag(volatility_vector)\n",
    "    \n",
    "    # Calculate the covariance matrix: Cov = D * Corr * D\n",
    "    cov_matrix = vol_diag @ correlation_matrix @ vol_diag\n",
    "    \n",
    "    return cov_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmahat(Y,k=None):\n",
    "    None, np.nan or int\n",
    "    #Pre-Conditions: Y is a valid pd.dataframe and optional arg- k which can be\n",
    "    #\n",
    "    #Post-Condition: Sigmahat dataframe is returned\n",
    "    #Set df dimensions\n",
    "    N = Y.shape[0]\n",
    "    p = Y.shape[1]\n",
    "    #default setting\n",
    "    if (k is None or math.isnan(k)):\n",
    "        Y = Y.sub(Y.mean(axis=0), axis=1)\n",
    "        k = 1\n",
    "    #vars\n",
    "    n = N-k\n",
    "    c = p/n\n",
    "    sample = pd.DataFrame(np.matmul(Y.T.to_numpy(),Y.to_numpy()))/n\n",
    "    sample = (sample+sample.T)/2\n",
    "    #make symmetrical\n",
    "    #Spectral decomp\n",
    "    lambda1, u = np.linalg.eigh(sample)\n",
    "    lambda1 = lambda1.real.clip(min=0)\n",
    "    dfu = pd.DataFrame(u,columns=lambda1) #create df with column names lambda\n",
    "    dfu.sort_index(axis=1,inplace = True)\n",
    "    lambda1 = dfu.columns\n",
    "    h = (min(c**2,1/c**2)**0.35)/p**0.35\n",
    "    #smoothing parameter\n",
    "    invlambda = 1/lambda1[max(1,p-n+1)-1:p] #inverse of (non-null) eigenvalues\n",
    "    dfl = pd.DataFrame()\n",
    "    dfl['lambda'] = invlambda\n",
    "    Lj = dfl[np.repeat(dfl.columns.values,min(p,n))]\n",
    "    Lj = pd.DataFrame(Lj.to_numpy())\n",
    "    Lj_i = Lj.subtract(Lj.T)\n",
    "    #like 1/lambda_j\n",
    "    #Reset column names\n",
    "    #like (1/lambda_j)-(1/lambda_i)\n",
    "    theta = Lj.multiply(Lj_i).div(Lj_i.multiply(Lj_i).add(Lj.multiply(Lj)*h**2)).mean(axis = 0)\n",
    "    #smoothed Stein shrinker\n",
    "    Htheta = Lj.multiply(Lj*h).div(Lj_i.multiply(Lj_i).add(Lj.multiply(Lj)*h**2)).mean(axis = 0)\n",
    "    Atheta2 = theta**2+Htheta**2\n",
    "    if p<=n:\n",
    "        delta = 1 / ((1-c)**2*invlambda+2*c*(1-c)*invlambda*theta \\\n",
    "        +c**2*invlambda*Atheta2)\n",
    "        delta = delta.to_numpy()\n",
    "    else:\n",
    "        delta0 = 1/((c-1)*np.mean(invlambda.to_numpy()))\n",
    "        delta = np.repeat(delta0,p-n)\n",
    "        delta = np.concatenate((delta, 1/(invlambda*Atheta2)), axis=None)\n",
    "    deltaQIS = delta*(sum(lambda1)/sum(delta))\n",
    "    temp1 = dfu.to_numpy()\n",
    "    temp2 = np.diag(deltaQIS)\n",
    "    temp3 = dfu.T.to_numpy().conjugate()\n",
    "    #reconstruct covariance matrix\n",
    "    #preserve trace\n",
    "    sigmahat = pd.DataFrame(np.matmul(np.matmul(temp1,temp2),temp3), index=Y.columns, columns=Y.columns)\n",
    "    return sigmahat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def minimum_variance_portfolio(cov_matrix):\n",
    "    \"\"\"\n",
    "    Calcule les pondérations du portefeuille de variance minimale\n",
    "    pour une matrice de covariance donnée. Si la matrice est mal conditionnée\n",
    "    ou non inversible, minimise directement la variance via une optimisation\n",
    "    avec une contrainte supplémentaire : aucun poids ne dépasse 0.10.\n",
    "    \"\"\"\n",
    "    n = len(cov_matrix)\n",
    "    ones = np.ones(n)\n",
    "\n",
    "    # Tenter l'inversion de la matrice de covariance\n",
    "    try:\n",
    "        inv_cov_matrix = np.linalg.inv(cov_matrix)\n",
    "        weights = inv_cov_matrix @ ones / (ones.T @ inv_cov_matrix @ ones)\n",
    "        \n",
    "        # Appliquer une contrainte post-inversion (pour les poids ≤ 0.10)\n",
    "        weights = np.clip(weights, 0, 0.10)\n",
    "        weights /= np.sum(weights)  # Re-normaliser pour que la somme soit égale à 1\n",
    "    except np.linalg.LinAlgError:\n",
    "        # Minimisation numérique si la matrice n'est pas inversible\n",
    "        def portfolio_variance(weights):\n",
    "            return weights.T @ cov_matrix @ weights\n",
    "\n",
    "        # Contrainte : Somme des poids = 1\n",
    "        constraints = ({'type': 'eq', 'fun': lambda weights: np.sum(weights) - 1})\n",
    "\n",
    "        # Les poids doivent être entre 0 et 0.10\n",
    "        bounds = [(0, 0.10) for _ in range(n)]\n",
    "\n",
    "        # Résolution du problème d'optimisation\n",
    "        result = minimize(portfolio_variance, \n",
    "                          x0=ones/n, \n",
    "                          bounds=bounds, \n",
    "                          constraints=constraints, \n",
    "                          method='SLSQP')\n",
    "\n",
    "        # Extraire les poids optimaux\n",
    "        if result.success:\n",
    "            weights = result.x\n",
    "        else:\n",
    "            raise ValueError(\"L'optimisation a échoué.\")\n",
    "\n",
    "    return weights\n",
    "\n",
    "def ERC_portfolio(forecasting):\n",
    "    cov=forecasting\n",
    "    n=len(cov.columns)\n",
    "    def objective(weights):\n",
    "        return(1/2*weights.T@cov@weights-1/n*np.sum(np.array([np.log(weights[i]) for i in range(n)])))\n",
    "    initial_weights=np.array([1/n+2]*n)\n",
    "    bounds=tuple((1,10) for _ in range(n))\n",
    "    result=minimize(objective, initial_weights,method='SLSQP', bounds=bounds,constraints=None)\n",
    "    weight=result.x/np.sum(result.x)\n",
    "    return(weight)\n",
    "\n",
    "def max_div_portfolio(forecasting):\n",
    "    cov=forecasting\n",
    "    n=len(cov.columns)\n",
    "    def objective(weights):\n",
    "        return(weights.T@cov@weights)\n",
    "    initial_weights=np.array([1/n]*n)\n",
    "    bounds=tuple((0,100) for _ in range(n))\n",
    "    constraints=({'type':'eq','fun': lambda weights:np.dot(weights, np.sqrt(np.diag(cov)))-1})\n",
    "    result=minimize(objective, initial_weights,method='SLSQP', bounds=bounds,constraints=constraints)\n",
    "    weight=result.x/np.sum(result.x)\n",
    "    return(weight)\n",
    "\n",
    "def portfolio_standard_deviation(returns, cov_matrices):\n",
    "    \"\"\"\n",
    "    Construit les portefeuilles de variance minimale à partir d'une liste\n",
    "    de matrices de covariance et calcule l'écart-type (standard deviation)\n",
    "    de la stratégie.\n",
    "    \n",
    "    :param returns: DataFrame avec les rendements journaliers par actifs.\n",
    "    :param cov_matrices: Liste de matrices de covariance.\n",
    "    :return: Écart-type de la stratégie de portefeuilles de variance minimale.\n",
    "    \"\"\"\n",
    "    n = len(returns)  # Nombre total de jours dans les rendements\n",
    "    L = len(cov_matrices)  # Nombre de matrices de covariance\n",
    "    X = n // L  # Nombre de jours pendant lesquels chaque portefeuille est maintenu\n",
    "    \n",
    "    portfolio_returns = []  # Liste pour stocker les rendements des portefeuilles\n",
    "\n",
    "    # Boucle sur chaque matrice de covariance pour créer un portefeuille\n",
    "    for i, cov_matrix in enumerate(cov_matrices):\n",
    "        # Calculer les pondérations du portefeuille de variance minimale\n",
    "        weights = minimum_variance_portfolio(cov_matrix)\n",
    "        \n",
    "        # Calculer les rendements du portefeuille pour les X jours correspondants\n",
    "        start = i * X\n",
    "        end = (i + 1) * X if (i + 1) * X <= n else n  # S'assurer de ne pas dépasser n\n",
    "        returns_subset = returns.iloc[start:end, :]  # Sous-ensemble des rendements\n",
    "        portfolio_return = returns_subset @ weights  # Rendement du portefeuille sur cette période\n",
    "        \n",
    "        # Ajouter les rendements du portefeuille à la liste\n",
    "        portfolio_returns.extend(portfolio_return)\n",
    "    \n",
    "    # Convertir en série pandas\n",
    "    portfolio_returns = pd.Series(portfolio_returns)\n",
    "    \n",
    "    # Calcul de l'écart-type de la stratégie de portefeuilles\n",
    "    strategy_std = portfolio_returns.std()\n",
    "    \n",
    "    return strategy_std*np.sqrt(252)\n",
    "\n",
    "def EWportfolio_standard_deviation(returns, cov_matrices, risk_free_rate=0.0):\n",
    "    \"\"\"\n",
    "    Construit les portefeuilles de variance minimale à partir d'une liste\n",
    "    de matrices de covariance et calcule l'écart-type (standard deviation)\n",
    "    de la stratégie.\n",
    "    \n",
    "    :param returns: DataFrame avec les rendements journaliers par actifs.\n",
    "    :param cov_matrices: Liste de matrices de covariance.\n",
    "    :return: Écart-type de la stratégie de portefeuilles de variance minimale.\n",
    "    \"\"\"\n",
    "    n = len(returns)  # Nombre total de jours dans les rendements\n",
    "    L = len(cov_matrices)  # Nombre de matrices de covariance\n",
    "    X = n // L  # Nombre de jours pendant lesquels chaque portefeuille est maintenu\n",
    "    print(X)\n",
    "    print(n)\n",
    "    portfolio_returns = []  # Liste pour stocker les rendements des portefeuilles\n",
    "\n",
    "    # Boucle sur chaque matrice de covariance pour créer un portefeuille\n",
    "    for i, cov_matrix in enumerate(cov_matrices):\n",
    "        # Calculer les pondérations du portefeuille de variance minimale\n",
    "        weights = np.ones(returns.shape[1]) / returns.shape[1]\n",
    "        \n",
    "        # Calculer les rendements du portefeuille pour les X jours correspondants\n",
    "        start = i * X\n",
    "        end = (i + 1) * X if (i + 1) * X <= n else n  # S'assurer de ne pas dépasser n\n",
    "        returns_subset = returns.iloc[start:end, :]  # Sous-ensemble des rendements\n",
    "        portfolio_return = returns_subset @ weights  # Rendement du portefeuille sur cette période\n",
    "        \n",
    "        # Ajouter les rendements du portefeuille à la liste\n",
    "        portfolio_returns.append((1 + portfolio_return).prod() - 1)\n",
    "\n",
    "    # Convertir en série pandas\n",
    "    portfolio_returns = pd.Series(portfolio_returns)\n",
    "\n",
    "    # Calcul de l'écart-type de la stratégie de portefeuilles (volatilité annualisée)\n",
    "    strategy_std = portfolio_returns.std() * np.sqrt(50)\n",
    "\n",
    "    # Calcul du rendement cumulé\n",
    "    cumulative_return = (1 + portfolio_returns).prod() - 1\n",
    "\n",
    "    # Calcul du rendement annualisé cumulé\n",
    "    annualized_return = (1 + cumulative_return) ** (50 / L) - 1\n",
    "\n",
    "    # Calcul du Sharpe ratio\n",
    "    excess_returns = portfolio_returns - (risk_free_rate / 50)\n",
    "    sharpe_ratio = (excess_returns.mean() * 50) / (strategy_std )\n",
    "\n",
    "    # Calcul du maximum drawdown\n",
    "    cumulative_returns = (1 + portfolio_returns).cumprod()\n",
    "    peak = cumulative_returns.cummax()\n",
    "    drawdown = (cumulative_returns - peak) / peak\n",
    "    max_drawdown = drawdown.min()\n",
    "\n",
    "    return {\n",
    "        'Annualized Standard Deviation': strategy_std,\n",
    "        'Max Drawdown': max_drawdown,\n",
    "        'Annualized Return': annualized_return,\n",
    "        'annualized mean return':excess_returns.mean() * 50,\n",
    "        'Sharpe Ratio': sharpe_ratio\n",
    "    }\n",
    "\n",
    "def portfolio_standard_deviation(returns, cov_matrices, risk_free_rate=0.0):\n",
    "    \"\"\"\n",
    "    Construit les portefeuilles de variance minimale à partir d'une liste\n",
    "    de matrices de covariance et calcule l'écart-type (standard deviation),\n",
    "    le maximum drawdown, le rendement annualisé cumulé, le Sharpe ratio,\n",
    "    la distance ERC et la diversification.\n",
    "\n",
    "    :param returns: DataFrame avec les rendements journaliers par actifs.\n",
    "    :param cov_matrices: Liste de matrices de covariance.\n",
    "    :param risk_free_rate: Taux sans risque pour calculer le Sharpe ratio.\n",
    "    :return: Un dictionnaire avec l'écart-type annualisé, le max drawdown, \n",
    "             le rendement annualisé cumulé, le Sharpe ratio, la distance ERC,\n",
    "             et la diversification.\n",
    "    \"\"\"\n",
    "    n = len(returns)  # Nombre total de jours dans les rendements\n",
    "    L = len(cov_matrices)  # Nombre de matrices de covariance\n",
    "    X = n // L  # Nombre de jours pendant lesquels chaque portefeuille est maintenu\n",
    "\n",
    "    portfolio_returns = []  # Liste pour stocker les rendements des portefeuilles\n",
    "    erc_distances = []  # Liste pour stocker les distances ERC\n",
    "    diversifications = []  # Liste pour stocker la diversification\n",
    "\n",
    "    # Boucle sur chaque matrice de covariance pour créer un portefeuille\n",
    "    for i, cov_matrix in enumerate(cov_matrices):\n",
    "        # Calculer les pondérations du portefeuille ERC\n",
    "        weights =ERC_portfolio(cov_matrix)\n",
    "\n",
    "        # Calculer les rendements du portefeuille pour les X jours correspondants\n",
    "        start = i * X\n",
    "        end = (i + 1) * X if (i + 1) * X <= n else n  # S'assurer de ne pas dépasser n\n",
    "        returns_subset = returns.iloc[start:end, :]  # Sous-ensemble des rendements\n",
    "        portfolio_return = returns_subset @ weights  # Rendement du portefeuille sur cette période\n",
    "\n",
    "        # Ajouter les rendements du portefeuille à la liste\n",
    "        portfolio_returns.append((1 + portfolio_return).prod() - 1)\n",
    "\n",
    "        # Calculer la volatilité totale du portefeuille\n",
    "        portfolio_volatility = portfolio_return.std()*np.sqrt(250)\n",
    "\n",
    "        # Contribution marginale au risque de chaque actif\n",
    "        marginal_risk_contributions = returns_subset.cov() @ weights / portfolio_volatility\n",
    "\n",
    "        # Contribution totale au risque de chaque actif\n",
    "        total_risk_contributions = weights * marginal_risk_contributions\n",
    "\n",
    "        # Moyenne des contributions totales au risque\n",
    "        mean_trc = np.mean(total_risk_contributions)\n",
    "\n",
    "        # Calcul de la distance ERC : somme des écarts absolus\n",
    "        erc_distance = np.sum(np.abs(total_risk_contributions - mean_trc))\n",
    "        erc_distances.append(erc_distance)\n",
    "\n",
    "        # Calcul de la diversification\n",
    "        asset_volatilities = np.sqrt(np.diag(returns_subset.cov()))  # Volatilité de chaque actif\n",
    "        diversification = (weights.T @ asset_volatilities) / portfolio_volatility\n",
    "        diversifications.append(diversification)\n",
    "\n",
    "    # Convertir en série pandas\n",
    "    portfolio_returns = pd.Series(portfolio_returns)\n",
    "\n",
    "    # Calcul de l'écart-type de la stratégie de portefeuilles (volatilité annualisée)\n",
    "    strategy_std = portfolio_returns.std() * np.sqrt(50)\n",
    "\n",
    "    # Calcul du rendement cumulé\n",
    "    cumulative_return = (1 + portfolio_returns).prod() - 1\n",
    "\n",
    "    # Calcul du rendement annualisé cumulé\n",
    "    annualized_return = (1 + cumulative_return) ** (50 / L) - 1\n",
    "\n",
    "    # Calcul du Sharpe ratio\n",
    "    excess_returns = portfolio_returns - (risk_free_rate / 50)\n",
    "    sharpe_ratio = (excess_returns.mean() * 50) / (strategy_std)\n",
    "\n",
    "    # Calcul du maximum drawdown\n",
    "    cumulative_returns = (1 + portfolio_returns).cumprod()\n",
    "    peak = cumulative_returns.cummax()\n",
    "    drawdown = (cumulative_returns - peak) / peak\n",
    "    max_drawdown = drawdown.min()\n",
    "\n",
    "    # Moyenne des distances ERC sur toutes les périodes\n",
    "    avg_erc_distance = np.mean(erc_distances)\n",
    "\n",
    "    # Moyenne de la diversification sur toutes les périodes\n",
    "    avg_diversification = np.mean(diversifications)\n",
    "\n",
    "    return {\n",
    "        'Annualized Standard Deviation': strategy_std,\n",
    "        'Max Drawdown': max_drawdown,\n",
    "        'Annualized Return': annualized_return,\n",
    "        'Annualized Mean Return': excess_returns.mean() * 50,\n",
    "        'Sharpe Ratio': sharpe_ratio,\n",
    "        'ERC Distance': avg_erc_distance,  # Ajout de la distance ERC\n",
    "        'Diversification': avg_diversification  # Ajout de la diversification\n",
    "    }\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Annualized Standard Deviation': 0.1901686043209643,\n",
       " 'Max Drawdown': -0.3708497799076201,\n",
       " 'Annualized Return': 0.09388400323505408,\n",
       " 'Annualized Mean Return': 0.10797615330565798,\n",
       " 'Sharpe Ratio': 0.5677916903855335,\n",
       " 'ERC Distance': 0.0005195253762846874,\n",
       " 'Diversification': 0.1533095005643699}"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "portfolio_standard_deviation(returns.iloc[4966:returns.shape[0]-1,:], Histo_cov21)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Annualized Standard Deviation': 0.20157466339036928,\n",
       " 'Max Drawdown': -0.3723194765890322,\n",
       " 'Annualized Return': 0.09220502384315044,\n",
       " 'Annualized Mean Return': 0.10855332407817102,\n",
       " 'Sharpe Ratio': 0.5385266295494031,\n",
       " 'ERC Distance': 0.0004946713400519605,\n",
       " 'Diversification': 0.14183499708880315}"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "portfolio_standard_deviation(returns.iloc[4966:returns.shape[0]-1,:], Histo_cov125)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Annualized Standard Deviation': 0.20060237484166465,\n",
       " 'Max Drawdown': -0.37231947658904063,\n",
       " 'Annualized Return': 0.09252479683249137,\n",
       " 'Annualized Mean Return': 0.10864888573028895,\n",
       " 'Sharpe Ratio': 0.5416131579501263,\n",
       " 'ERC Distance': 0.0004958530687788928,\n",
       " 'Diversification': 0.142970476482191}"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "portfolio_standard_deviation(returns.iloc[4966:returns.shape[0]-1,:], Ewma_cov)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Annualized Standard Deviation': 0.20086637651533054,\n",
       " 'Max Drawdown': -0.37051216472831355,\n",
       " 'Annualized Return': 0.09226710201851751,\n",
       " 'Annualized Mean Return': 0.10845769650698622,\n",
       " 'Sharpe Ratio': 0.5399494847695852,\n",
       " 'ERC Distance': 0.0004956245966942916,\n",
       " 'Diversification': 0.14248742019925292}"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "portfolio_standard_deviation(returns.iloc[4966:returns.shape[0]-1,:], reg_cov)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "portfolio_standard_deviation(returns.iloc[4966:returns.shape[0]-1,:], NL21_cov)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Annualized Standard Deviation': 0.17007556964843656,\n",
       " 'Max Drawdown': -0.3139740601221061,\n",
       " 'Annualized Return': 0.10708473122507378,\n",
       " 'Annualized Mean Return': 0.11625809505634006,\n",
       " 'Sharpe Ratio': 0.6835672830416345,\n",
       " 'ERC Distance': 0.000837310502840542,\n",
       " 'Diversification': 0.15275411484927634}"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "portfolio_standard_deviation(returns.iloc[4966:returns.shape[0]-1,:], NL125_cov)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[62], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mportfolio_standard_deviation\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreturns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miloc\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m4966\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43mreturns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mNL125_cov\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[61], line 192\u001b[0m, in \u001b[0;36mportfolio_standard_deviation\u001b[1;34m(returns, cov_matrices, risk_free_rate)\u001b[0m\n\u001b[0;32m    189\u001b[0m \u001b[38;5;66;03m# Boucle sur chaque matrice de covariance pour créer un portefeuille\u001b[39;00m\n\u001b[0;32m    190\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, cov_matrix \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(cov_matrices):\n\u001b[0;32m    191\u001b[0m     \u001b[38;5;66;03m# Calculer les pondérations du portefeuille ERC\u001b[39;00m\n\u001b[1;32m--> 192\u001b[0m     weights \u001b[38;5;241m=\u001b[39m\u001b[43mmax_div_portfolio\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcov_matrix\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    194\u001b[0m     \u001b[38;5;66;03m# Calculer les rendements du portefeuille pour les X jours correspondants\u001b[39;00m\n\u001b[0;32m    195\u001b[0m     start \u001b[38;5;241m=\u001b[39m i \u001b[38;5;241m*\u001b[39m X\n",
      "Cell \u001b[1;32mIn[61], line 64\u001b[0m, in \u001b[0;36mmax_div_portfolio\u001b[1;34m(forecasting)\u001b[0m\n\u001b[0;32m     62\u001b[0m bounds\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mtuple\u001b[39m((\u001b[38;5;241m0\u001b[39m,\u001b[38;5;241m100\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n))\n\u001b[0;32m     63\u001b[0m constraints\u001b[38;5;241m=\u001b[39m({\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtype\u001b[39m\u001b[38;5;124m'\u001b[39m:\u001b[38;5;124m'\u001b[39m\u001b[38;5;124meq\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfun\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28;01mlambda\u001b[39;00m weights:np\u001b[38;5;241m.\u001b[39mdot(weights, np\u001b[38;5;241m.\u001b[39msqrt(np\u001b[38;5;241m.\u001b[39mdiag(cov)))\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m})\n\u001b[1;32m---> 64\u001b[0m result\u001b[38;5;241m=\u001b[39m\u001b[43mminimize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobjective\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minitial_weights\u001b[49m\u001b[43m,\u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mSLSQP\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbounds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbounds\u001b[49m\u001b[43m,\u001b[49m\u001b[43mconstraints\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconstraints\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     65\u001b[0m weight\u001b[38;5;241m=\u001b[39mresult\u001b[38;5;241m.\u001b[39mx\u001b[38;5;241m/\u001b[39mnp\u001b[38;5;241m.\u001b[39msum(result\u001b[38;5;241m.\u001b[39mx)\n\u001b[0;32m     66\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m(weight)\n",
      "File \u001b[1;32mc:\\Users\\33640\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\scipy\\optimize\\_minimize.py:722\u001b[0m, in \u001b[0;36mminimize\u001b[1;34m(fun, x0, args, method, jac, hess, hessp, bounds, constraints, tol, callback, options)\u001b[0m\n\u001b[0;32m    719\u001b[0m     res \u001b[38;5;241m=\u001b[39m _minimize_cobyla(fun, x0, args, constraints, callback\u001b[38;5;241m=\u001b[39mcallback,\n\u001b[0;32m    720\u001b[0m                            bounds\u001b[38;5;241m=\u001b[39mbounds, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions)\n\u001b[0;32m    721\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m meth \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mslsqp\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m--> 722\u001b[0m     res \u001b[38;5;241m=\u001b[39m \u001b[43m_minimize_slsqp\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfun\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjac\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbounds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    723\u001b[0m \u001b[43m                          \u001b[49m\u001b[43mconstraints\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    724\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m meth \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrust-constr\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m    725\u001b[0m     res \u001b[38;5;241m=\u001b[39m _minimize_trustregion_constr(fun, x0, args, jac, hess, hessp,\n\u001b[0;32m    726\u001b[0m                                        bounds, constraints,\n\u001b[0;32m    727\u001b[0m                                        callback\u001b[38;5;241m=\u001b[39mcallback, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions)\n",
      "File \u001b[1;32mc:\\Users\\33640\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\scipy\\optimize\\_slsqp_py.py:437\u001b[0m, in \u001b[0;36m_minimize_slsqp\u001b[1;34m(func, x0, args, jac, bounds, constraints, maxiter, ftol, iprint, disp, eps, callback, finite_diff_rel_step, **unknown_options)\u001b[0m\n\u001b[0;32m    431\u001b[0m slsqp(m, meq, x, xl, xu, fx, c, g, a, acc, majiter, mode, w, jw,\n\u001b[0;32m    432\u001b[0m       alpha, f0, gs, h1, h2, h3, h4, t, t0, tol,\n\u001b[0;32m    433\u001b[0m       iexact, incons, ireset, itermx, line,\n\u001b[0;32m    434\u001b[0m       n1, n2, n3)\n\u001b[0;32m    436\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:  \u001b[38;5;66;03m# objective and constraint evaluation required\u001b[39;00m\n\u001b[1;32m--> 437\u001b[0m     fx \u001b[38;5;241m=\u001b[39m \u001b[43mwrapped_fun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    438\u001b[0m     c \u001b[38;5;241m=\u001b[39m _eval_constraint(x, cons)\n\u001b[0;32m    440\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m:  \u001b[38;5;66;03m# gradient evaluation required\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\33640\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\scipy\\optimize\\_optimize.py:300\u001b[0m, in \u001b[0;36m_clip_x_for_func.<locals>.eval\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m    294\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_clip_x_for_func\u001b[39m(func, bounds):\n\u001b[0;32m    295\u001b[0m     \u001b[38;5;66;03m# ensures that x values sent to func are clipped to bounds\u001b[39;00m\n\u001b[0;32m    296\u001b[0m \n\u001b[0;32m    297\u001b[0m     \u001b[38;5;66;03m# this is used as a mitigation for gh11403, slsqp/tnc sometimes\u001b[39;00m\n\u001b[0;32m    298\u001b[0m     \u001b[38;5;66;03m# suggest a move that is outside the limits by 1 or 2 ULP. This\u001b[39;00m\n\u001b[0;32m    299\u001b[0m     \u001b[38;5;66;03m# unclean fix makes sure x is strictly within bounds.\u001b[39;00m\n\u001b[1;32m--> 300\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21meval\u001b[39m(x):\n\u001b[0;32m    301\u001b[0m         x \u001b[38;5;241m=\u001b[39m _check_clip_x(x, bounds)\n\u001b[0;32m    302\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(x)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "portfolio_standard_deviation(returns.iloc[4966:returns.shape[0]-1,:], NL125_cov)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "Histo_cov21=[]\n",
    "Histo_cov125=[]\n",
    "Ewma_cov=[]\n",
    "dccgarch_cov=[]\n",
    "reg_cov=[]\n",
    "NL21_cov=[]\n",
    "NL125_cov=[]\n",
    "halflife_pairs=[(10, 21), (21, 63), (63, 125)]\n",
    "for t in range(4966, returns.shape[0]-1, 5):\n",
    "    if t%50==0:\n",
    "        print(t)\n",
    "    i=0\n",
    "    Histo_cov21.append(histo_cov(returns, t, 21))\n",
    "    Histo_cov125.append(histo_cov(returns, t, 125))\n",
    "    Ewma_cov.append(ewma_cov(returns.iloc[:,:50], t, halflife_pairs, 500))\n",
    "    reg_cov.append(covariance_matrix(reg_RW_vols(returns, t, [5,21]), ewma_correlation(returns, t, 0.97)))\n",
    "    NL21_cov.append(sigmahat(returns.iloc[t-21:t,:],k=None))\n",
    "    NL125_cov.append(sigmahat(returns.iloc[t-125:t,:],k=None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ewma_cov(df, date, window, beta):\n",
    "    cov=np.zeros((len(df.columns),len(df.columns)))\n",
    "    for i in range(date-window+1, date+1):\n",
    "        cov=(beta)*cov+(1-beta)*np.outer(np.array(df.iloc[i,:]),np.array(df.iloc[i,:]))\n",
    "    covs=pd.DataFrame(cov*250, columns=df.columns, index=df.columns)\n",
    "    return(covs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "Ewma_cov=[]\n",
    "for t in range(4966, returns.shape[0]-1, 5):\n",
    "    Ewma_cov.append(ewma_cov(returns, t, 500, 0.96))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
