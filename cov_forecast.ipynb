{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "import math\n",
    "import seaborn as sns\n",
    "from cvx.covariance.combination import from_ewmas\n",
    "from arch import arch_model\n",
    "from statsmodels.tsa.vector_ar.vecm import coint_johansen\n",
    "from scipy.optimize import minimize\n",
    "from statsmodels.tools.tools import add_constant\n",
    "from statsmodels.regression.linear_model import OLS\n",
    "from sklearn.linear_model import Ridge, Lasso\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import r2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv(r'C:\\Users\\33640\\OneDrive\\Documents\\GitHub\\Covariance-Correlation-Volatility-Forecasting\\SP500_daily_returns.csv')\n",
    "nan_counts=df.isna().sum()\n",
    "columns_to_keep = nan_counts[nan_counts <= 3].index\n",
    "returns = df[columns_to_keep]\n",
    "returns=returns.dropna()\n",
    "returns.shape\n",
    "dates=returns['Date'] \n",
    "returns=returns.drop('Date', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def histo_cov(df, date, window):\n",
    "    cov=np.zeros((len(df.columns),len(df.columns)))\n",
    "    for i in range(date-window+1, date+1):\n",
    "        cov+=np.outer(np.array(df.iloc[i,:]),np.array(df.iloc[i,:]))/(window)\n",
    "    covs=pd.DataFrame(cov*250, columns=df.columns, index=df.columns)\n",
    "    return(covs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "halflife_pairs=[(10, 21), (21, 63), (63, 125)]\n",
    "def ewma_cov(df, date, halflife_pairs, window=250):    \n",
    "    combinator = from_ewmas(df.iloc[max(0,date-1000):date,:],\n",
    "                                halflife_pairs,\n",
    "                                min_periods_vola=window,  # min periods for volatility estimation\n",
    "                                min_periods_cov=window)  # min periods for correlation estimation\n",
    "\n",
    "    # Solve combination problem and loop through combination results to get predictors\n",
    "    covariance_predictors = {}\n",
    "    for predictor in combinator.solve(window=10):  # lookback window for optimization\n",
    "        # From predictor we can access predictor.time, predictor.mean (=0 here),\n",
    "        # predictor.covariance, and predictor.weights\n",
    "        covariance_predictors[predictor.time] = predictor.covariance\n",
    "    return(covariance_predictors[date]*250)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Fit GARCH(1,1) to each asset's returns\n",
    "def fit_garch(returns):\n",
    "    # Rescaler les rendements\n",
    "    returns_rescaled = returns * 100\n",
    "    \n",
    "    model = arch_model(returns_rescaled, vol='Garch', p=1, q=1)\n",
    "    garch_fit = model.fit(disp=\"off\")\n",
    "    \n",
    "    # Revenir à l'échelle d'origine pour les résidus\n",
    "    residuals = garch_fit.resid / 100\n",
    "    volatilities = garch_fit.conditional_volatility / 100\n",
    "    return volatilities, residuals\n",
    "# Step 2: DCC-GARCH log-likelihood function\n",
    "def dcc_garch_log_likelihood(params, residuals):\n",
    "    \"\"\"\n",
    "    Calcule la log-vraisemblance du modèle DCC-GARCH.\n",
    "    \n",
    "    Paramètres:\n",
    "    params (tuple): Paramètres alpha_2 et beta_2.\n",
    "    residuals (np.array): Matrice des résidus standardisés (T x N).\n",
    "    \n",
    "    Retourne:\n",
    "    float: La valeur de la log-vraisemblance négative.\n",
    "    \"\"\"\n",
    "    alpha_2, beta_2 = params\n",
    "    n_assets = residuals.shape[1]\n",
    "    \n",
    "    # Initialisation de Q_bar et Q_t\n",
    "    T = residuals.shape[0]\n",
    "    Q_bar = np.cov(residuals.T)  # Matrice de corrélation inconditionnelle\n",
    "    Q_t = Q_bar.copy()\n",
    "    \n",
    "    log_likelihood = 0\n",
    "    epsilon = 1e-5  # Petit terme de régularisation pour éviter les matrices singulières\n",
    "    \n",
    "    for t in range(T):\n",
    "        # Mise à jour de Q_t selon la formule DCC\n",
    "        Q_t = (1 - alpha_2 - beta_2) * Q_bar + alpha_2 * np.outer(residuals[t], residuals[t]) + beta_2 * Q_t\n",
    "        \n",
    "        # Régularisation de la diagonale de Q_t pour éviter les valeurs négatives\n",
    "        diag_Q_t = np.diag(Q_t)\n",
    "        diag_Q_t = np.clip(diag_Q_t, epsilon, None)  # Clip des valeurs négatives ou proches de zéro\n",
    "        Q_t = np.diag(diag_Q_t) + (Q_t - np.diag(np.diag(Q_t)))  # Remplacer la diagonale\n",
    "        \n",
    "        # Ajouter un petit terme de régularisation à Q_t\n",
    "        Q_t += epsilon * np.eye(n_assets)\n",
    "        \n",
    "        # Normaliser Q_t pour obtenir R_t (matrice de corrélation dynamique)\n",
    "        D_t = np.diag(1 / np.sqrt(np.diag(Q_t)))\n",
    "        R_t = D_t @ Q_t @ D_t\n",
    "        \n",
    "        # Vérifier les valeurs propres de R_t et corriger si nécessaire\n",
    "        eigenvalues = np.linalg.eigvals(R_t)\n",
    "        if np.any(eigenvalues <= 0):\n",
    "            R_t += epsilon * np.eye(n_assets)\n",
    "        \n",
    "        # Calcul de la log-vraisemblance pour chaque étape t\n",
    "        try:\n",
    "            term1 = np.log(np.linalg.det(R_t))  # Log du déterminant de R_t\n",
    "        except np.linalg.LinAlgError:\n",
    "            # En cas de matrice singulière\n",
    "            return np.inf  # Retourner une grande valeur pour pénaliser l'optimisation\n",
    "        \n",
    "        term2 = residuals[t].T @ np.linalg.pinv(R_t) @ residuals[t]  # epsilon_t' R_t^{-1} epsilon_t\n",
    "        term3 = residuals[t].T @ residuals[t]  # epsilon_t' epsilon_t\n",
    "        log_likelihood += term1 + term2 + term3  # Ajouter les trois termes pour chaque t\n",
    "    \n",
    "    # Retourner la log-vraisemblance négative\n",
    "    return 0.5 * log_likelihood\n",
    "\n",
    "# Step 6: Optimization of alpha_2 and beta_2\n",
    "def optimize_dcc_garch(returns):\n",
    "    \"\"\"\n",
    "    Optimise les paramètres alpha_2 et beta_2 du modèle DCC-GARCH en utilisant une fenêtre de données.\n",
    "    \n",
    "    Paramètres:\n",
    "    returns (pd.DataFrame): DataFrame des rendements journaliers des actifs (chaque colonne représente un actif).\n",
    "    window (str): Fenêtre temporelle pour la sélection des données ('6M' pour 6 mois, '1Y' pour 1 an, etc.).\n",
    "    \n",
    "    Retourne:\n",
    "    tuple: Paramètres optimisés alpha_2 et beta_2.\n",
    "    \"\"\"\n",
    "    n_assets = returns.shape[1]\n",
    "    \n",
    "    # Ajuster GARCH(1,1) sur chaque actif et standardiser les résidus\n",
    "    volatilities = np.zeros_like(returns)\n",
    "    residuals = np.zeros_like(returns)\n",
    "    \n",
    "    for i in range(n_assets):\n",
    "        volatilities[:, i], residuals[:, i] = fit_garch(returns.iloc[:, i])\n",
    "    \n",
    "    standardized_residuals = residuals / volatilities\n",
    "    \n",
    "    # Valeurs initiales pour alpha_2 et beta_2\n",
    "    initial_params = np.array([0.05, 0.9])\n",
    "    \n",
    "    # Contraintes sur les paramètres\n",
    "    bounds = [(0, 1), (0, 0.95)]  # Limiter beta_2 à 0.95 pour éviter une convergence lente\n",
    "    \n",
    "    # Minimiser la log-vraisemblance négative\n",
    "    result = minimize(dcc_garch_log_likelihood, initial_params, args=(standardized_residuals), bounds=bounds, method='SLSQP', options={'disp': True})\n",
    "    \n",
    "    if result.success:\n",
    "        optimized_params = result.x\n",
    "        return optimized_params\n",
    "    else:\n",
    "        raise ValueError(\"Optimization failed\")\n",
    "\n",
    "def predict_covariance_dcc_garch(df, date, window, horizon=1):\n",
    "    \"\"\"\n",
    "    Calcule la matrice de covariance prédite par le modèle DCC-GARCH à un horizon donné.\n",
    "    \n",
    "    Paramètres:\n",
    "    returns (pd.DataFrame): DataFrame des rendements journaliers des actifs (chaque colonne représente un actif).\n",
    "    window (str): Fenêtre temporelle pour la sélection des données ('6M' pour 6 mois, '1Y' pour 1 an, etc.).\n",
    "    horizon (int): Nombre de jours dans le futur pour lesquels la covariance doit être prédite.\n",
    "    \n",
    "    Retourne:\n",
    "    pd.DataFrame: Matrice de covariance prédite à l'horizon spécifié.\n",
    "    \"\"\"\n",
    "    returns=df.iloc[date-window:date,:]\n",
    "    n_assets = returns.shape[1]\n",
    "    \n",
    "    # Ajuster GARCH(1,1) sur chaque actif et standardiser les résidus\n",
    "    volatilities = np.zeros_like(returns)\n",
    "    residuals = np.zeros_like(returns)\n",
    "    \n",
    "    for i in range(n_assets):\n",
    "        volatilities[:, i], residuals[:, i] = fit_garch(returns.iloc[:, i])\n",
    "    \n",
    "    standardized_residuals = residuals / volatilities\n",
    "    \n",
    "    # Initialiser les paramètres DCC-GARCH avec les valeurs optimisées\n",
    "    optimized_alpha2, optimized_beta2 = optimize_dcc_garch(returns)\n",
    "    \n",
    "    # Initialisation de Q_bar et Q_t\n",
    "    Q_bar = np.cov(standardized_residuals.T)\n",
    "    Q_t = Q_bar.copy()\n",
    "    T = standardized_residuals.shape[0]\n",
    "    \n",
    "    # Utiliser les paramètres optimisés pour prédire la covariance\n",
    "    for t in range(T):\n",
    "        Q_t = (1 - optimized_alpha2 - optimized_beta2) * Q_bar + optimized_alpha2 * np.outer(standardized_residuals[t], standardized_residuals[t]) + optimized_beta2 * Q_t\n",
    "    \n",
    "    # Normaliser Q_t pour obtenir la matrice de corrélation dynamique R_t à la dernière date\n",
    "    D_t = np.diag(1 / np.sqrt(np.diag(Q_t)))\n",
    "    R_t = D_t @ Q_t @ D_t\n",
    "    \n",
    "    # Prédire la matrice de covariance à horizon k jours\n",
    "    last_volatilities = volatilities[-1, :]\n",
    "    D_last = np.diag(last_volatilities)\n",
    "    predicted_covariance = D_last @ R_t @ D_last\n",
    "    \n",
    "    # Boucle pour étendre la prédiction à horizon k jours\n",
    "    for _ in range(horizon):\n",
    "        # Mettre à jour Q_t pour chaque jour à venir\n",
    "        Q_t = (1 - optimized_alpha2 - optimized_beta2) * Q_bar + optimized_beta2 * Q_t\n",
    "        \n",
    "        # Normaliser Q_t pour obtenir R_t (corrélation dynamique après k jours)\n",
    "        D_t = np.diag(1 / np.sqrt(np.diag(Q_t)))\n",
    "        R_t = D_t @ Q_t @ D_t\n",
    "        \n",
    "        # Mise à jour des volatilités pour chaque jour\n",
    "        predicted_covariance = D_last @ R_t @ D_last\n",
    "    \n",
    "    # Retourner la matrice de covariance sous forme de DataFrame avec les colonnes et index des actifs\n",
    "    return pd.DataFrame(predicted_covariance*250, index=returns.columns, columns=returns.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reg_RW_vols(df, date, windows):\n",
    "    Har_vols = []\n",
    "    for actif in df.columns:\n",
    "        vols=[[] for i in range(len(windows))]\n",
    "        vol_realized = []\n",
    "        M=max(windows)\n",
    "        for t in range(date-250, date ):\n",
    "            for i, window in enumerate(windows):\n",
    "                if window>1:\n",
    "                    vols[i].append(df.loc[t - window+1:t, actif].std() * np.sqrt(250))\n",
    "                else:\n",
    "                    vols[i].append(np.sqrt(df.loc[t , actif]**2 * 250))\n",
    "            vol_realized.append(np.sqrt(df.loc[t+1 , actif]**2 * 250))\n",
    "        vol_data = pd.DataFrame({f'vol_{i+1}': vols[i] for i in range(len(windows))})\n",
    "        vol_data['vol_realized'] = vol_realized\n",
    "        vol_data = vol_data.dropna()\n",
    "        \n",
    "        X = vol_data[[f'vol_{i+1}' for i in range(len(windows))]]\n",
    "        X = add_constant(X)\n",
    "        y = vol_data['vol_realized']\n",
    "        model = OLS(y, X).fit()\n",
    "        latest_vols={}\n",
    "        latest_vols['intercept'] = [1]\n",
    "        for i, window in enumerate(windows):\n",
    "            if window>1:\n",
    "                latest_vols[f'vol_{i+1}']=[df.loc[date - window+1:date, actif].std() * np.sqrt(250)] \n",
    "            else:\n",
    "                latest_vols[f'vol_{i+1}']=[np.sqrt(df.loc[date, actif]**2 * np.sqrt(250))] \n",
    "        \n",
    "        latest_data = pd.DataFrame(latest_vols)\n",
    "        vol_forecast = model.predict(latest_data)\n",
    "        Har_vols.append(vol_forecast.iloc[0])\n",
    "    \n",
    "    return (np.array(Har_vols))\n",
    "\n",
    "\n",
    "def ewma_correlation_matrix(returns_df, decay_factor=0.94):\n",
    "    # Returns DataFrame: daily returns with assets in columns and days in rows\n",
    "    # decay_factor: smoothing factor for EWMA (lambda in EWMA formula)\n",
    "    \n",
    "    # Calculate EWMA variance-covariance matrix\n",
    "    cov_matrix = returns_df.ewm(span=1/(1-decay_factor)).cov()\n",
    "\n",
    "    # Extract the last covariance matrix (most recent one)\n",
    "    last_cov_matrix = cov_matrix.iloc[-returns_df.shape[1]:]\n",
    "    \n",
    "    # Calculate standard deviations from the covariance matrix\n",
    "    std_devs = np.sqrt(np.diag(last_cov_matrix))\n",
    "    \n",
    "    # Create correlation matrix by dividing covariances by std deviations\n",
    "    corr_matrix = last_cov_matrix.div(std_devs, axis=0).div(std_devs, axis=1)\n",
    "    \n",
    "    return corr_matrix\n",
    "\n",
    "def covariance_matrix(volatility_vector, correlation_matrix):\n",
    "    # volatility_vector: 1D numpy array or list of volatilities (standard deviations)\n",
    "    # correlation_matrix: 2D numpy array or DataFrame of correlations\n",
    "    \n",
    "    # Convert the volatility vector into a diagonal matrix\n",
    "    vol_diag = np.diag(volatility_vector)\n",
    "    \n",
    "    # Calculate the covariance matrix: Cov = D * Corr * D\n",
    "    cov_matrix = vol_diag @ correlation_matrix @ vol_diag\n",
    "    \n",
    "    return cov_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmahat(Y,k=None):\n",
    "    None, np.nan or int\n",
    "    #Pre-Conditions: Y is a valid pd.dataframe and optional arg- k which can be\n",
    "    #\n",
    "    #Post-Condition: Sigmahat dataframe is returned\n",
    "    #Set df dimensions\n",
    "    N = Y.shape[0]\n",
    "    p = Y.shape[1]\n",
    "    #default setting\n",
    "    if (k is None or math.isnan(k)):\n",
    "        Y = Y.sub(Y.mean(axis=0), axis=1)\n",
    "        k = 1\n",
    "    #vars\n",
    "    n = N-k\n",
    "    c = p/n\n",
    "    sample = pd.DataFrame(np.matmul(Y.T.to_numpy(),Y.to_numpy()))/n\n",
    "    sample = (sample+sample.T)/2\n",
    "    #make symmetrical\n",
    "    #Spectral decomp\n",
    "    lambda1, u = np.linalg.eigh(sample)\n",
    "    lambda1 = lambda1.real.clip(min=0)\n",
    "    dfu = pd.DataFrame(u,columns=lambda1) #create df with column names lambda\n",
    "    dfu.sort_index(axis=1,inplace = True)\n",
    "    lambda1 = dfu.columns\n",
    "    h = (min(c**2,1/c**2)**0.35)/p**0.35\n",
    "    #smoothing parameter\n",
    "    invlambda = 1/lambda1[max(1,p-n+1)-1:p] #inverse of (non-null) eigenvalues\n",
    "    dfl = pd.DataFrame()\n",
    "    dfl['lambda'] = invlambda\n",
    "    Lj = dfl[np.repeat(dfl.columns.values,min(p,n))]\n",
    "    Lj = pd.DataFrame(Lj.to_numpy())\n",
    "    Lj_i = Lj.subtract(Lj.T)\n",
    "    #like 1/lambda_j\n",
    "    #Reset column names\n",
    "    #like (1/lambda_j)-(1/lambda_i)\n",
    "    theta = Lj.multiply(Lj_i).div(Lj_i.multiply(Lj_i).add(Lj.multiply(Lj)*h**2)).mean(axis = 0)\n",
    "    #smoothed Stein shrinker\n",
    "    Htheta = Lj.multiply(Lj*h).div(Lj_i.multiply(Lj_i).add(Lj.multiply(Lj)*h**2)).mean(axis = 0)\n",
    "    Atheta2 = theta**2+Htheta**2\n",
    "    if p<=n:\n",
    "        delta = 1 / ((1-c)**2*invlambda+2*c*(1-c)*invlambda*theta \\\n",
    "        +c**2*invlambda*Atheta2)\n",
    "        delta = delta.to_numpy()\n",
    "    else:\n",
    "        delta0 = 1/((c-1)*np.mean(invlambda.to_numpy()))\n",
    "        delta = np.repeat(delta0,p-n)\n",
    "        delta = np.concatenate((delta, 1/(invlambda*Atheta2)), axis=None)\n",
    "    deltaQIS = delta*(sum(lambda1)/sum(delta))\n",
    "    temp1 = dfu.to_numpy()\n",
    "    temp2 = np.diag(deltaQIS)\n",
    "    temp3 = dfu.T.to_numpy().conjugate()\n",
    "    #reconstruct covariance matrix\n",
    "    #preserve trace\n",
    "    sigmahat = pd.DataFrame(np.matmul(np.matmul(temp1,temp2),temp3), index=Y.columns, columns=Y.columns)\n",
    "    return sigmahat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def minimum_variance_portfolio(cov_matrix):\n",
    "    \"\"\"\n",
    "    Calcule les pondérations du portefeuille de variance minimale\n",
    "    pour une matrice de covariance donnée. Si la matrice est mal conditionnée\n",
    "    ou non inversible, minimise directement la variance via une optimisation\n",
    "    avec une contrainte supplémentaire : aucun poids ne dépasse 0.10.\n",
    "    \"\"\"\n",
    "    n = len(cov_matrix)\n",
    "    ones = np.ones(n)\n",
    "\n",
    "    # Tenter l'inversion de la matrice de covariance\n",
    "    try:\n",
    "        inv_cov_matrix = np.linalg.inv(cov_matrix)\n",
    "        weights = inv_cov_matrix @ ones / (ones.T @ inv_cov_matrix @ ones)\n",
    "        \n",
    "        # Appliquer une contrainte post-inversion (pour les poids ≤ 0.10)\n",
    "        weights = np.clip(weights, 0, 0.10)\n",
    "        weights /= np.sum(weights)  # Re-normaliser pour que la somme soit égale à 1\n",
    "    except np.linalg.LinAlgError:\n",
    "        # Minimisation numérique si la matrice n'est pas inversible\n",
    "        def portfolio_variance(weights):\n",
    "            return weights.T @ cov_matrix @ weights\n",
    "\n",
    "        # Contrainte : Somme des poids = 1\n",
    "        constraints = ({'type': 'eq', 'fun': lambda weights: np.sum(weights) - 1})\n",
    "\n",
    "        # Les poids doivent être entre 0 et 0.10\n",
    "        bounds = [(0, 0.10) for _ in range(n)]\n",
    "\n",
    "        # Résolution du problème d'optimisation\n",
    "        result = minimize(portfolio_variance, \n",
    "                          x0=ones/n, \n",
    "                          bounds=bounds, \n",
    "                          constraints=constraints, \n",
    "                          method='SLSQP')\n",
    "\n",
    "        # Extraire les poids optimaux\n",
    "        if result.success:\n",
    "            weights = result.x\n",
    "        else:\n",
    "            raise ValueError(\"L'optimisation a échoué.\")\n",
    "\n",
    "    return weights\n",
    "\n",
    "def ERC_portfolio(df,forecasting):\n",
    "    cov=forecasting\n",
    "    n=len(df.columns)\n",
    "    def objective(weights):\n",
    "        return(1/2*weights.T@cov@weights-1/n*np.sum(np.array([np.log(weights[i]) for i in range(n)])))\n",
    "    initial_weights=np.array([1/n+0.01]*n)\n",
    "    bounds=tuple((0,100) for _ in range(n))\n",
    "    result=minimize(objective, initial_weights,method='SLSQP', bounds=bounds,constraints=None)\n",
    "    weight=result.x/np.sum(result.x)\n",
    "    return(weight)\n",
    "\n",
    "def max_div_portfolio(df,forecasting):\n",
    "    cov=forecasting\n",
    "    n=len(df.columns)\n",
    "    def objective(weights):\n",
    "        return(weights.T@cov@weights)\n",
    "    initial_weights=np.array([1/n]*n)\n",
    "    bounds=tuple((0,100) for _ in range(n))\n",
    "    constraints=({'type':'eq','fun': lambda weights:np.dot(weights, np.sqrt(np.diag(cov)))-1})\n",
    "    result=minimize(objective, initial_weights,method='SLSQP', bounds=bounds,constraints=constraints)\n",
    "    weight=result.x/np.sum(result.x)\n",
    "    return(weight)\n",
    "\n",
    "def portfolio_standard_deviation(returns, cov_matrices):\n",
    "    \"\"\"\n",
    "    Construit les portefeuilles de variance minimale à partir d'une liste\n",
    "    de matrices de covariance et calcule l'écart-type (standard deviation)\n",
    "    de la stratégie.\n",
    "    \n",
    "    :param returns: DataFrame avec les rendements journaliers par actifs.\n",
    "    :param cov_matrices: Liste de matrices de covariance.\n",
    "    :return: Écart-type de la stratégie de portefeuilles de variance minimale.\n",
    "    \"\"\"\n",
    "    n = len(returns)  # Nombre total de jours dans les rendements\n",
    "    L = len(cov_matrices)  # Nombre de matrices de covariance\n",
    "    X = n // L  # Nombre de jours pendant lesquels chaque portefeuille est maintenu\n",
    "    \n",
    "    portfolio_returns = []  # Liste pour stocker les rendements des portefeuilles\n",
    "\n",
    "    # Boucle sur chaque matrice de covariance pour créer un portefeuille\n",
    "    for i, cov_matrix in enumerate(cov_matrices):\n",
    "        # Calculer les pondérations du portefeuille de variance minimale\n",
    "        weights = minimum_variance_portfolio(cov_matrix)\n",
    "        \n",
    "        # Calculer les rendements du portefeuille pour les X jours correspondants\n",
    "        start = i * X\n",
    "        end = (i + 1) * X if (i + 1) * X <= n else n  # S'assurer de ne pas dépasser n\n",
    "        returns_subset = returns.iloc[start:end, :]  # Sous-ensemble des rendements\n",
    "        portfolio_return = returns_subset @ weights  # Rendement du portefeuille sur cette période\n",
    "        \n",
    "        # Ajouter les rendements du portefeuille à la liste\n",
    "        portfolio_returns.extend(portfolio_return)\n",
    "    \n",
    "    # Convertir en série pandas\n",
    "    portfolio_returns = pd.Series(portfolio_returns)\n",
    "    \n",
    "    # Calcul de l'écart-type de la stratégie de portefeuilles\n",
    "    strategy_std = portfolio_returns.std()\n",
    "    \n",
    "    return strategy_std*np.sqrt(252)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Histo_cov21=[]\n",
    "Histo_cov125=[]\n",
    "ewma_cov=[]\n",
    "dccgarch_cov=[]\n",
    "reg_cov=[]\n",
    "NL21_cov=[]\n",
    "NL125_cov=[]\n",
    "halflife_pairs=[(10, 21), (21, 63), (63, 125)]\n",
    "for t in range(4966, returns.shape[0]-1):\n",
    "    if t%50==0:\n",
    "        print(t)\n",
    "    i=0\n",
    "    histo21=histo_cov(returns, t, 21)\n",
    "    histo125=histo_cov(returns, t, 125)\n",
    "    ewma94=ewma_cov(returns, t, halflife_pairs, 250)\n",
    "    garch=forecast_volatility_garch(returns, t, horizon=1)\n",
    "    har=reg_RW_vols(returns, t, [1,5,21])\n",
    "    reg=reg_model_vols(returns, t, [1,5,21])\n",
    "    for actif in returns.columns:\n",
    "        const_vol[i].append(C[i])\n",
    "        Realized_vol[i].append(realized[i])\n",
    "        histo_vol1[i].append(histo1[i])\n",
    "        histo_vol5[i].append(histo5[i])\n",
    "        ewma_vol94[i].append(ewma94[i])\n",
    "        garch_vol[i].append(garch[i])\n",
    "        har_vol[i].append(har[i])\n",
    "        reg_vol[i].append(reg[i])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
